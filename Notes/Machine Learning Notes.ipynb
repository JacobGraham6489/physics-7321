{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85760285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================== DIFFERENTIATION CHEAT SHEET (NO FRAMEWORKS) ===========================\n",
    "# Notation:\n",
    "# - Scalars:      x ∈ R\n",
    "# - Vectors:      x ∈ R^N \n",
    "# - Covectors:    w ∈ (R^N)* (dual space)\n",
    "# - Matrices:     X ∈ R^{N×M}\n",
    "# - Tensors:      Θ ∈ R^{N1×N2×…}  (multi-dimensional array)\n",
    "# - Flatten:      vec(Θ) ∈ R^P with P = ∏_k Nk   (reshaping does not change derivatives; only layout of input; row-major convention by default (stack rows left to right) but any consistent flattening works)\n",
    "#\n",
    "# Note on shape\n",
    "# - R^d            = a length-d 1D array (shape (d,)); no row/column orientation is encoded (in NumPy/JAX/PyTorch, vectors and covectors are both stored as 1D arrays). \n",
    "# - R^{1×d}        = row vector  (shape (1, d)).\n",
    "# - R^{d×1}        = column vector (shape (d, 1)).\n",
    "# - R^{m×n}        = matrix (2D array, shape (m, n)); rows = m, cols = n.\n",
    "# - Tensors        = multi-dim arrays Θ ∈ R^{N₁×…×N_k} with shape (N₁,…,N_k).\n",
    "# - Flattening     = vec(Θ) reshapes Θ to 1D (shape (∏ₗ Nₗ,)); reshaping doesn’t change derivative values, only layout.\n",
    "# - Gradients      = match the primal’s shape (e.g., ∇_X F has the same shape as X). “Column vector” is a math convention, not a stored shape.\n",
    "# - Broadcasting   = elementwise ops allow size 1 to expand along an axis (libraries follow NumPy broadcasting rules).\n",
    "#\n",
    "# 1) GRADIENTS (SCALAR OUTPUT)\n",
    "# --------------------------------------------------------------------------------\n",
    "# 1.1) f : R → R\n",
    "#   Derivative: df/dx ∈ R.\n",
    "#\n",
    "# 1.2) F : R^n → R   (scalar-valued function on a vector)\n",
    "#   Math:    ∇_x F(x) = [∂F/∂x₁, …, ∂F/∂xₙ]ᵀ ∈ R^{n×1} (column vector)\n",
    "#   Code:    ∇_x F(x) = [∂F/∂x₁, …, ∂F/∂xₙ]  ∈ R^n (row/column orientation isn’t encoded)\n",
    "#  \n",
    "# 1.3) F : R^{N₁×…×N_k} → R   (scalar-valued function on a tensor)\n",
    "#   Math/Code: (∇_X F)[i₁,…,i_k] = ∂F/∂X[i₁,…,i_k], so ∇_X F ∈ R^{N₁×…×N_k} (componentwise definition on input tensor X[i₁,…,i_k])\n",
    "#   Flattening equivalence: if vec(X) ∈ R^P with P = ∏ₗ Nₗ, then\n",
    "#       vec(∇_X F) = ∇_{vec(X)} F.  (Same entries; only reshaped.)\n",
    "#\n",
    "#  In each case, the gradients have the same shape as the primal input x or X. If x or X is passed in as a row/column vector or reshaped tensor, the gradient matches that shape.\n",
    "#\n",
    "# Example (applies to 1.2 and 1.3):\n",
    "#   F(X) = ½‖X‖² = ½ Σ_{all indices} X[idx]²\n",
    "#   ⇒ ∇_X F = X  (same shape as X).  Flattening yields ∇_{vec(X)} F = vec(X).\n",
    "#\n",
    "# --------------------------------------------------------------------------------\n",
    "# 2) JACOBIANS (VECTOR OUTPUT)\n",
    "# --------------------------------------------------------------------------------\n",
    "# 2.1) G : R^n → R^m  (vector → vector)\n",
    "# Math:\n",
    "#   J_x G(x) ∈ R^{m×n}, with entries J_x G(x)[i, j] = ∂G_i/∂x_j.\n",
    "#   Row i is the gradient of the i-th output:  J_x G(x)[i, :] = (∇_x G_i(x))^T ∈ R^{1×n}.\n",
    "# Code:\n",
    "#   J_x G(x)[i, j] = ∂G_i/∂x_j ∈ R^{m×n}. \n",
    "#   J_x G(x)[i, :] = ∇_x G_i(x) ∈ R^n \n",
    "#\n",
    "# 2.2) G : R^{N1×…×Nk} → R^m  (tensor → vector)\n",
    "# Math/Code:\n",
    "# J_X G(X) ∈ R^{m × N1 × … × Nk}, with entries J_X G(X)_{p, i1,...,ik} = ∂G_p(X) / ∂X_{i1,...,ik},    for p = 1..m.\n",
    "# Equivalently: the Jacobian is a stack of gradient tensors\n",
    "#   ∇_X G_p(X) ∈ R^{N1×…×Nk}   (one gradient tensor per output p) and J_G is {∇_X G_p(X)}_p stacked along a new leading dimension p.\n",
    "# \n",
    "# Flattened equivalence (convenient for some routines like linear algebra):\n",
    "# Let P = ∏_ℓ Nℓ and vec(X) ∈ R^P be a flattening of X (e.g., row-major). Let lin(i_1,...,i_k) ∈ {0,1,..,P-1}  be the linear index of X[i_1,...,i_k] under this flattening, so Vec(X)[lin(i_1,...,i_k)] = X[i_1,...,i_k].\n",
    "# Then, J_X G(X) ∈ R^{m x P} where, J_X G(X)[p, lin(i1,...,ik)] = ∂G_p(X) / ∂X_{i1,...,ik} for p = 1..m.\n",
    "# Row p is the gradient of the pth output: J_X G(X)[p, :] = vec(∇_X G_p(X)) ∈ R^P\n",
    "#\n",
    "# Example (X ∈ R^{N1×…×Nk}, A ∈ R^{m×N1×…×Nk} and G_p = <A_p∘ X> ∈ R (inner product over all tensor indices)):\n",
    "# G_p(X) = ⟨A_p, X⟩ = Σ_{i1,...,ik} A_{p;i1,...,ik} X_{i1,...,ik}, then ∂G_p/∂X_{i1,...,ik} = A_{p;i1,...,ik}  ⇒  ∇_X G_p(X) = A_p, and J_G ∈ R^{m x P} with J_X G(X) [p, :] = vec(∇_X G_p(X)) ∈ R^P\n",
    "#\n",
    "# 2.3) G : (R^{N1×…×Nk} × R^{M1×…×Mr}) → R^m  (multi-arg: inputs X and params θ; just applying linearity)\n",
    "# Math/Code:\n",
    "#   J_G(X, θ) = [ J_X G(X, θ), J_θ G(X, θ) ] (returns two block matrices), where\n",
    "#   J_X G(X, θ) ∈ R^{m × N1 × … × Nk},   with entries J_X G(X, θ)[p, i1,...,ik] = ∂G_p/∂X_{i1,...,ik},\n",
    "#   J_θ G(X, θ) ∈ R^{m × M1 × … × Mr},   with entries J_θ G(X, θ)[p, j1,...,jl] = ∂G_p/∂θ_{j1,...,jl}.\n",
    "#\n",
    "# Flattened equivalence:\n",
    "#   J_G(X, θ) = [ J_X G(X, θ)   J_θ G(X, θ) ] ∈ R^{m × (Px + Pθ)} (concatenated), where\n",
    "#   J_X G(X, θ) ∈ R^{m×Px},  Px = ∏ Nℓ,   rows = vec(∇_X G_i(X)) ∈ R^Px\n",
    "#   J_θ G(X, θ) ∈ R^{m×Pθ},  Pθ = ∏ Mj,   rows = vec(∇_θ G_i(θ)) ∈ R^Pθ\n",
    "#\n",
    "# Note that all flattening is done manually by reshaping the output tensors\n",
    "#\n",
    "# --------------------------------------------------------------------------------\n",
    "# 3) Pushforwards (JVPs) and Pullbacks (VJPs)\n",
    "# --------------------------------------------------------------------------------\n",
    "# Setup:\n",
    "#   F : (X × Θ) → Y, with x ∈ X  (dim X = N), θ ∈ Θ  (dim θ = P), y = F(x, θ) ∈ Y (dim Y = M).\n",
    "#   Let {x^i} (i=1..N), {θ^j} (j=1..P), {y^k} (k=1..M) be coordinates on X, Θ, Y respectively.\n",
    "#   Write J_x F and J_θ F for the partial Jacobians w.r.t. x and θ: J_x F[k, i] = ∂F^k/∂x^i(x,θ),   J_θ F[k, j] = ∂F^k/∂θ^j(x,θ).\n",
    "#\n",
    "# Pushforward (differential) at (x, θ):\n",
    "#   dF_(x,θ) : T_(x,θ) [X × Θ] = T_x X × T_θ Θ → T_y Y.\n",
    "# Given tangents v_x ∈ T_x X and v_θ ∈ T_θ Θ, the pushed-forward tangent is (denoting the restriction of dF_(x,θ) to each tangent space T_x X and T_θ Θ as dF_x and dF_θ respectively):\n",
    "#   dF_(x,θ)[v_x, v_θ] = dF_(x,θ)[v_x] + dF_(x,θ)[v_θ] = dF_x[v_x] + dF_θ[v_θ]  ∈ T_y Y ->  [dF_(x,θ)[v_x, v_θ]]^k = (∂F^k/∂x^i)(x,θ) v_x^i  + (∂F^k/∂θ^j)(x,θ) v_θ^j =  [J_x F(x,θ) · v_x]^k +  [J_θ F(x,θ) · v_θ]^k\n",
    "#   (v_x and v_θ are arrays with components v_x^i and v_θ^j respectively, so they are treated as column vectors in these matrix products by the matrix multiplication conventions above)\n",
    "# Thus,  dF_(x,θ)[v_x, v_θ] = J_x F(x,θ) · v_x  +  J_θ F(x,θ) · v_θ, i.e., the sum of two \"Jacobian–vector products\" (JVPs).\n",
    "# \n",
    "# Pullback at (x, θ):\n",
    "#   F^*_(x,θ) : T*_y Y → T*_(x,θ) [X × Θ] = T*_x X × T*_θ Θ.\n",
    "# Given a cotangent w ∈ T*_y Y, the pulled-back covector is defined on tangents v_x ∈ T_x X and v_θ ∈ T_θ Θ by:\n",
    "#   F^*_(x,θ)(w)[v_x, v_θ] = w · [ dF_(x,θ)[v_x, v_θ] ] = w[dF_x[v_x] + dF_θ[v_θ]] = w_k (∂F^k/∂x^i)(x,θ) v_x^i +  w_k (∂F^k/∂θ^j)(x,θ) v_θ^j = [J_x F(x,θ)^T · w]_i v_x^i + [J_θ F(x,θ)^T · w]_j v_θ^j\n",
    "#   (w is an array with components w_k, so it is treated as a column vector in these matrix products by the matrix multiplication conventions above)\n",
    "# Thus, F^*_(x,θ)(w) = (x̄, θ̄) ∈ T*_x X × T*_θ Θ, with x̄_i = [J_x F(x,θ)^T · w]_i and θ̄_j = [J_θ F(x,θ)^T · w]_j, i.e., a tuple of \"vector–Jacobian products\" (VJPs).\n",
    "#\n",
    "# ---------------------------------------------------------------------------------\n",
    "# ADDING A SCALAR READOUT φ : Y → R AND GRADIENTS VIA VJP (BACKPROP)\n",
    "# ------------------------------------------------------------------------------\n",
    "# Setup:\n",
    "#   • y = F(x, θ) ∈ Y, with F : X × Θ → Y.\n",
    "#   • φ : Y → R is any smooth scalar readout on Y.\n",
    "#   • Define the scalar objective on X×Θ:  Φ(x, θ) := (φ ∘ F)(x, θ).\n",
    "#\n",
    "# Output covector seed at y\n",
    "# -------------------------\n",
    "#   • Let w := (dφ)_y ∈ T*_y Y be the differential (row-like covector) of φ at y.\n",
    "#     In coordinates {y^k}, this is w_k = ∂φ/∂y^k (evaluated at y).\n",
    "#\n",
    "# Chain rule (pullback through F)\n",
    "# -------------------------------\n",
    "#   • Differential of Φ at (x, θ) is the pullback of w:\n",
    "#       dΦ_(x,θ) = F^*_(x,θ)(w) ∈ T*_(x,θ)(X×Θ) = T*_x X x T*_θ Θ (dΦ_(x,θ) = dφ_y ∘ dF_(x,θ) = F^*_(x,θ)(w))\n",
    "#     In blocks:\n",
    "#       x̄ := J_x F(x,θ)^T · w   ∈ T*_x X,\n",
    "#       θ̄ := J_θ F(x,θ)^T · w   ∈ T*_θ Θ,\n",
    "#     so dΦ_(x,θ)[v_x, v_θ] = dΦ_x[v_x] + dΦ_θ[v_θ] = x̄(v_x) + θ̄(v_θ).\n",
    "#\n",
    "# Coordinate formulae (indices)\n",
    "# -----------------------------\n",
    "#   • Let J_x F[k,i] = ∂F^k/∂x^i and J_θ F[k,j] = ∂F^k/∂θ^j.\n",
    "#     Then with w_k = ∂φ/∂y^k:\n",
    "#       (∂Φ/∂x^i) = (J_x F)^T_{i k} w_k = ∑_k (∂F^k/∂x^i) (∂φ/∂y^k),\n",
    "#       (∂Φ/∂θ^j) = (J_θ F)^T_{j k} w_k = ∑_k (∂F^k/∂θ^j) (∂φ/∂y^k).\n",
    "#\n",
    "# Vector gradients via metrics \n",
    "# ---------------------------------------\n",
    "#   • If X, Θ, Y carry Riemannian metrics g_X, g_Θ, g_Y, the vector gradients are “sharps”:\n",
    "#\n",
    "#     ∇_x Φ = (d_x Φ)^# -> (∇_x Φ)^i = g_X^{i k} (d_x Φ)_k  = g_X^{i k} x̄_k -> ∇_x Φ = g_X^{-1} x̄,\n",
    "#     ∇_θ Φ = (d_θ Φ)^# -> (∇_θ Φ)^j = g_Θ^{j l} (d_θ Φ)_l  = g_Θ^{j l} θ̄_l  -> ∇_θ Φ = g_Θ^{-1} θ̄\n",
    "#\n",
    "#   • In Euclidean spaces (identity metrics): \n",
    "#       ∇_x Φ = J_x F^T w,      ∇_θ Φ = J_θ F^T w.\n",
    "#\n",
    "#   Thus, To get gradients of the scalar objective Φ w.r.t. inputs/parameters, pull back the output covector w = dφ_y through F (VJP) to obtain (x̄, θ̄), then raise indices via the metric(s) to convert covectors to vectors.\n",
    "# --------------------------------------------------------------------------------\n",
    "# 4) JVPs and VJPs for general multivariate vector functions\n",
    "# --------------------------------------------------------------------------------\n",
    "# Setup: y = F(X, θ) ∈ R^M, with inputs X and (optional) parameters θ (can be tensors).\n",
    "# Goal\n",
    "# - JVP (forward sensitivity):    given a tangent in input/param space, compute J_(X, θ) F(X, θ) · u (i.e., the pushforward dF_{X,θ}(u) = directional derivative of F at (X,θ) along u)\n",
    "# - VJP (reverse sensitivity):    given a cotangent in output space, compute J_(X, θ) F(X, θ)^T · w. (i.e., the pullback F^*(w)_{X,θ} = w ∘ dF_{X,θ})\n",
    "# These are the core primitives behind forward-mode and reverse-mode autodiff.\n",
    "#\n",
    "# ---------------------------------------\n",
    "# 4.1) JVP — Jacobian–vector product\n",
    "# ---------------------------------------\n",
    "# Input: tangents (u_X, u_θ) matching the shapes of (X, θ).\n",
    "# Output: JVP = J_X F(X,θ) · u_X  +  J_θ F(X,θ) · u_θ   ∈ R^M (the pushforward of (u_X, u_θ) through F)\n",
    "#\n",
    "# Cases:\n",
    "# • F: R^N → R^M\n",
    "#     - x, u_x ∈ R^N  → J_x F(x) ∈ R^{M×N} →  JVP = J_x F(x) · u_x ∈ R^M.\n",
    "# • F: R^{N1×…×Nk} → R^M\n",
    "#     - X, u_X ∈ R^{N1×…×Nk} → J_X F(X) ∈ R^{M×N1×…×Nk} → JVP = J_X F(X) · u_X ∈ R^M.\n",
    "# • F: (X, θ) → R^M\n",
    "#     - X, u_X ∈ R^{N1×…×Nk}; θ, u_θ ∈ R^{M1×…×Mr} →  J_X F(X, θ) ∈ R^{M×N1×…×Nk}, J_θ F(X, θ) ∈ R^{M×M1×…×Mr} →  JVP = J_X F(X,θ) · u_X  +  J_θ F(X,θ) · u_θ  ∈ R^M \n",
    "#     - If you only care about input sensitivity, set u_θ = 0 (and vice versa). \n",
    "#\n",
    "# Intuition:\n",
    "# - Push a small input/parameter change forward through F to get the corresponding first-order change in the output.\n",
    "#\n",
    "# ---------------------------------------\n",
    "# 4.2) VJP — Vector–Jacobian product (backprop)\n",
    "# ---------------------------------------\n",
    "# Input: output-space cotangent w ∈ R^M (same shape as y = F(X, θ)).\n",
    "# Output: a tuple of pullbacks (x̄, θ̄):\n",
    "#     x̄   = J_X F(X,θ)^T · w       (same shape as X)\n",
    "#     θ̄   = J_θ F(X,θ)^T · w       (same shape(s) as θ; one tensor per θ block)\n",
    "#\n",
    "# Cases:\n",
    "# • F: R^N → R^M\n",
    "#     - x ∈ R^N  → J_x F(x) ∈ R^{M×N} →  VJP = J_x F(x)^T · w ∈ R^N.\n",
    "# • F: R^{N1×…×Nk} → R^M\n",
    "#     - X ∈ R^{N1×…×Nk} → J_X F(X) ∈ R^{M×N1×…×Nk} → VJP = J_X F(X)^T · w ∈ R^{N1×…×Nk}.\n",
    "# • F: (X, θ) → R^M\n",
    "#     - X ∈ R^{N1×…×Nk}; θ ∈ R^{M1×…×Mr} →  J_X F(X, θ) ∈ R^{M×N1×…×Nk}, J_θ F(X, θ) ∈ R^{M×M1×…×Mr} →  VJP = [J_X F(X,θ)^T · w  ∈ R^{N1×…×Nk}, J_θ F(X,θ)^T · w ∈ R^{M1×…×Mr}] \n",
    "#\n",
    "# Intuition:\n",
    "# - Pull a covector w at the output back through F to obtain gradients with respect to inputs/parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c630dc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT IS A NEURAL NETWORK?\n",
    "# -----------------------------------------------------------------------------\n",
    "# A neural network is a parametric function f_θ that maps inputs to outputs:\n",
    "#   f_θ : R^{d_in} → R^{d_out},  x ↦ ŷ = f_θ(x) (equivalently f: (x; θ) ∈ R^{d_in} × R^P  ↦ ŷ = f(x; θ))\n",
    "# - x ∈ R^{d_in}: input vector of features (numeric descriptors). ŷ \n",
    "# - ŷ ∈ R^{d_out}: output vector (predictions).\n",
    "# - θ ∈ R^P : all learnable parameters of the model, \"learnable” means θ is adjusted during training to minimize a scalar loss L(θ) via optimization (e.g., gradient descent).\n",
    "#\n",
    "# Note: \n",
    "# • When we write x ∈ R^{d_in}, ŷ ∈ R^{d_out},  θ ∈ R^P, we mean the flattened (vectorized) collection vec(x), vec(ŷ), vec(θ) have length d_in, d_out, and P respectively.\n",
    "#   In practice, x, ŷ, θ can be a scalar, vector, matrix, higher-order tensor, or a nested collection thereof.\n",
    "#\n",
    "# Three regression examples for x (features) → ŷ (targets):\n",
    "#   1) Housing prices:                  x = [square_footage, num_bedrooms, year_built],         ŷ = [price] (scalar)\n",
    "#   2) Physics (oscillator):            x = [t]  (time), or x = [A, β, ω, δ],                   ŷ = [x(t)] or [x(t) for many t]\n",
    "#   3) Physics (experiment):            x = [temperature, pressure, field_strength, …],         ŷ = [measured_signal(t) for t = t_0, t_1, …, t_{T-1}]\n",
    "#\n",
    "# Shapes:\n",
    "#   - Single feature:                                            x ∈ R^{d_in},                  ŷ ∈ R^{d_out}\n",
    "#   - Batch of B features: {x_i} for i = 0,...,B-1:              X ∈ R^{B×d_in},                Ŷ ∈ R^{B×d_out}   (each row is one example)\n",
    "#\n",
    "# LAYERS (WEIGHTS, BIASES, ACTIVATIONS)\n",
    "# -----------------------------------------------------------------------------\n",
    "# A NN is built using a composition of L \"layers\" labeled ℓ = 0,...,L-1: \n",
    "#\n",
    "# θ := (θ_0, θ_1, …, θ_{L-1}) # whole-model parameters (one block per layer)\n",
    "# f(; θ) = f_{L-1}(; θ_{L-1})∘ f_{L-2}(; θ_{L-2}) ∘ … ∘ f_0(; θ_0) # whole network as composition of layer functions\n",
    "#\n",
    "# Forward (single example):\n",
    "#   h_0 := x ∈ R^{d_0}                              # input\n",
    "#   for ℓ = 0,…,L-1:\n",
    "#       h_{ℓ+1} = f_ℓ(h_ℓ ; θ_ℓ) ∈ R^{d_{ℓ+1}}      # layer-ℓ transform with its own parameters θ_ℓ\n",
    "#   ŷ = h_L ∈ R^{d_L} (d_L = d_out)                 # output\n",
    "#\n",
    "# (Batch, row-major): X ∈ R^{B×d_0}, H_ℓ ∈ R^{B×d_ℓ}, H_{ℓ+1} = f_ℓ(H_ℓ ; θ_ℓ) ∈ R^{B×d_{ℓ+1}}, Ŷ = H_L ∈ R^{B×d_L}\n",
    "#\n",
    "# Width of layer ℓ = d_ℓ (number of units/channels in that layer)\n",
    "# Depth of NN = L (number of layers)\n",
    "#\n",
    "# WEIGHTS, BIASES, ACTIVATIONS (the building blocks of each layer)\n",
    "# -------------------------------------------------------------------------------\n",
    "# DENSE (FULLY CONNECTED) LAYER = AFFINE MAP + (OPTIONAL) NONLINEARITY \n",
    "# We define:\n",
    "#   - feature:               h_0  := input to first layer (i.e. x)\n",
    "#   - activation:            h_ℓ  := output of layer ℓ (input to layer ℓ+1)\n",
    "#   - logit:                 z_ℓ  := pre-activation output of layer ℓ (pure affine transform θ_ℓ := (W_ℓ,b_ℓ) of h_ℓ)\n",
    "#   - weight:                W_ℓ  := layer ℓ weight (matrix)\n",
    "#   - bias:                  b_ℓ  := layer ℓ bias (translation vector)\n",
    "#   - activation function:   σ    := nonlinear function (acts elementwise and preserves shape σ : R^{...×d_{ℓ+1}} → R^{...×d_{ℓ+1}})\n",
    "#   - layer ℓ function:    f_ℓ(; θ_ℓ)  := σ ∘ (W_ℓ, b_ℓ)  (affine map followed by nonlinearity)\n",
    "#\n",
    "# Notation & shapes:\n",
    "# - In code, features h_0 ∈ R^{d_0}, activations h_ℓ ∈ R^{d_ℓ}, biases b_ℓ ∈ R^{d_{ℓ+1}}, and logits z_ℓ ∈ R^{d_{ℓ+1}} (1D arrays).\n",
    "# - Batches of B features H_0 ∈ R^{B×d_ℓ}, activations H_ℓ ∈ R^{B×d_ℓ}, and logits Z_ℓ ∈ R^{B×d_{ℓ+1}} (2D arrays, rows are individual samples); biases b_ℓ ∈ R^{d_{ℓ+1}} still 1D and broadcast across rows.\n",
    "# - Weights W_ℓ are 2D arrays/matrices, shape depends on framework convention (see below).\n",
    "#\n",
    "# ---------------------------------- PyTorch Convention ----------------------------------\n",
    "# Storage (matches torch.nn.Linear):\n",
    "#   W_ℓ ∈ R^{d_{ℓ+1} × d_ℓ}      # (out, in)\n",
    "#\n",
    "# Single feature/activation as 1D input:\n",
    "#   input h_ℓ ∈ R^{d_ℓ}\n",
    "#   logit z_ℓ = W_ℓ @ h_ℓ + b_ℓ             # z_ℓ ∈ R^{d_{ℓ+1}}\n",
    "#   output h_{ℓ+1} = σ(z_ℓ)                  # h_{ℓ+1} ∈ R^{d_{ℓ+1}}\n",
    "#\n",
    "# Single feature/activation as a ROW (keep 2D):\n",
    "#   input h_ℓ_row ∈ R^{1 × d_ℓ}\n",
    "#   logit z_ℓ_row = h_ℓ_row @ W_ℓ^T + b_ℓ   # z_ℓ_row ∈ R^{1 × d_{ℓ+1}}\n",
    "#   output h_{ℓ+1,row} = σ(z_ℓ_row)          # h_{ℓ+1,row} ∈ R^{1 × d_{ℓ+1}}\n",
    "#\n",
    "# Batch of B features/activations (rows are samples):\n",
    "#   input  H_ℓ ∈ R^{B × d_ℓ}\n",
    "#   logits Z_ℓ = H_ℓ @ W_ℓ^T + b_ℓ           # Z_ℓ ∈ R^{B × d_{ℓ+1}}  (b_ℓ broadcasts on rows)\n",
    "#   output H_{ℓ+1} = σ(Z_ℓ)                  # H_{ℓ+1} ∈ R^{B × d_{ℓ+1}}\n",
    "#\n",
    "# ------------------------------------ JAX Convention ------------------------------------\n",
    "# Storage (transpose-free for row-major batches):\n",
    "#   W_ℓ ∈ R^{d_ℓ × d_{ℓ+1}}      # (in, out)\n",
    "#\n",
    "# Single feature/activation as 1D input:\n",
    "#   input  h_ℓ ∈ R^{d_ℓ}\n",
    "#   logit  z_ℓ = h_ℓ @ W_ℓ + b_ℓ             # z_ℓ ∈ R^{d_{ℓ+1}}\n",
    "#   output h_{ℓ+1} = σ(z_ℓ)                  # h_{ℓ+1} ∈ R^{d_{ℓ+1}}\n",
    "#\n",
    "# Single feature/activation as a ROW (2D):\n",
    "#   input  h_ℓ_row ∈ R^{1 × d_ℓ}\n",
    "#   logit  z_ℓ_row = h_ℓ_row @ W_ℓ + b_ℓ     # z_ℓ_row ∈ R^{1 × d_{ℓ+1}}\n",
    "#   output h_{ℓ+1,row} = σ(z_ℓ_row)          # h_{ℓ+1,row} ∈ R^{1 × d_{ℓ+1}}\n",
    "#\n",
    "# Batch of B features/activations (rows are samples):\n",
    "#   input  H_ℓ ∈ R^{B × d_ℓ}\n",
    "#   logits Z_ℓ = H_ℓ @ W_ℓ + b_ℓ             # Z_ℓ ∈ R^{B × d_{ℓ+1}}  (b_ℓ broadcasts on rows)\n",
    "#   output H_{ℓ+1} = σ(Z_ℓ)                  # H_{ℓ+1} ∈ R^{B × d_{ℓ+1}}\n",
    "#\n",
    "# COMMON ACTIVATION FUNCTIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "#     identity(z) = z\n",
    "#       - No nonlinearity; used at output heads for regression tasks.\n",
    "#\n",
    "#     ReLU(z) = max(0, z)\n",
    "#       - Elementwise on logits \n",
    "#       - Piecewise linear, zero for negatives, identity for positives.\n",
    "#       - Pros: simple, fast, sparse activations; strong default.\n",
    "#       - Cons: “dead ReLU” (units can get stuck at 0).\n",
    "#\n",
    "#     tanh(z) = (e^z - e^{-z}) / (e^z + e^{-z})\n",
    "#       - Elementwise on logits\n",
    "#       - Smooth, bounded in (-1, 1), zero-centered.\n",
    "#       - Pros: good for smoothly varying signals.\n",
    "#       - Cons: can saturate for large |z| → small gradients.\n",
    "#\n",
    "#     sigmoid(z) = 1 / (1 + e^{-z})\n",
    "#       - Elementwise on logits\n",
    "#       - Smooth, bounded in (0, 1).\n",
    "#       - Use mainly at binary-classification output heads (probabilities).\n",
    "#       - Cons: saturates at extremes; not ideal for hidden layers.\n",
    "#\n",
    "#     GELU(z) = z * Φ(z),  Φ = standard normal CDF\n",
    "#       - Elementwise on logits\n",
    "#       - Common approx: 0.5 * z * (1 + tanh(√(2/π) * (z + 0.044715 z^3)))\n",
    "#       - Smooth “probabilistic gate”; small negatives softly down-weighted.\n",
    "#       - Strong default in transformers/deep nets.\n",
    "#\n",
    "#    Swish(z) = z * sigmoid(z)\n",
    "#       - Smooth, non-monotonic; similar spirit to GELU.\n",
    "#       - Often performs slightly better than ReLU in some settings; a bit slower.\n",
    "#\n",
    "#   Softmax(z)_i = exp(z_i) / Σ_j exp(z_j) (Boltzmann distribution)\n",
    "#       - Not elementwise (acts across a vector); used at multi-class output heads\n",
    "#       - to convert logits z to class probabilities (softmax(z) ∈ [0,1]^{d_out}, Σ_i softmax(z)_i = 1)\n",
    "# \n",
    "# INTUITION\n",
    "# - Weights (W): combine/rotate/scale input features to form new features.\n",
    "# - Biases  (b): shift each output unit’s activation threshold.\n",
    "# - Activation σ: injects nonlinearity; without it, stacked layers collapse to one affine map.\n",
    "#\n",
    "# THREE REGRESSION EXAMPLES\n",
    "# -----------------------------------------------------------------------------\n",
    "# Notation: for hidden width h, input dim d_in, output dim d_out (= task-specific).\n",
    "# A dense layer applies: h_out = σ(W @ h_in + b). Shapes shown batch-first where useful.\n",
    "#\n",
    "# 1) Housing prices\n",
    "#    x = [square_footage, num_bedrooms, year_built] ∈ R^{d_in=3} , output ŷ ∈ R^{d_out=1} (scalar price)\n",
    "#    Suggested MLP (multi-layer perceptron): 3 → h=64 → 1\n",
    "#    Weights/Biases:\n",
    "#      W1 ∈ R^{64×3},  b1 ∈ R^{64}\n",
    "#      W2 ∈ R^{1×64},  b2 ∈ R^{1}\n",
    "#    Activations:\n",
    "#      hidden σ: ReLU or tanh (e.g., h1 = σ(W1 x + b1))\n",
    "#    Output head:\n",
    "#      identity (no σ): ŷ = W2 h1 + b2  ∈ R^{1}   # regression scalar \n",
    "#\n",
    "# 2) Physics (oscillator)\n",
    "#    Option A (time → position): x = [t] ∈ R^{1}, output ŷ = x(t) ∈ R^{1} (scalar position at time t)\n",
    "#      MLP: 1 → h=32 → 1\n",
    "#      Weights/Biases: W1 ∈ R^{32×1}, b1 ∈ R^{32}; W2 ∈ R^{1×32}, b2 ∈ R^{1}\n",
    "#      Activations: hidden σ: tanh (smooth signals)\n",
    "#      Output head: identity (no σ): ŷ = W2 h1 + b2     # regression scalar \n",
    "#\n",
    "#    Option B (params → full waveform): x = [A, β, ω, δ] ∈ R^{4}, output ŷ ∈ R^{T} (vector of positions at T timepoints)\n",
    "#      MLP: 4 → h=64 → T\n",
    "#      Weights/Biases: W1 ∈ R^{64×4}, b1 ∈ R^{64}; W2 ∈ R^{T×64}, b2 ∈ R^{T}\n",
    "#      Activations: hidden σ: tanh/ReLU\n",
    "#      Output head: identity (no σ) ŷ = W2 h1 + b2  # vector regression over timepoints\n",
    "#\n",
    "# 3) Physics (experiment mapping env vars → measured signal(s))\n",
    "#    x = [temperature, pressure, field_strength, …] ∈ R^{P}, output ŷ ∈ R^{T} (vector of measured_signal(t) at T timepoints)\n",
    "#    Suggested MLP: p → h=128 → h=64 → m\n",
    "#    Weights/Biases:\n",
    "#      W1 ∈ R^{128×p},  b1 ∈ R^{128}\n",
    "#      W2 ∈ R^{64×128}, b2 ∈ R^{64}\n",
    "#      W3 ∈ R^{m×64},   b3 ∈ R^{m}\n",
    "#    Activations:\n",
    "#      hidden σ: GELU/ReLU (nonlinear instrument response)\n",
    "#    Output head:\n",
    "#      identity (no σ): ŷ = W3 h2 + b3  # multi-output regression (m channels)\n",
    "#\n",
    "# NOTE: Not all NN layers are \"dense (affine) + nonlinearity\".\n",
    "# - Examples:\n",
    "#     • Convolutions: local, weight-shared linear ops + activation (not a full W @ x).\n",
    "#     • Attention: content-based mixing (softmax(QK^T / √d) V) + projections/MLPs.\n",
    "#     • Recurrent cells (LSTM/GRU): gated state updates; parameters used across time.\n",
    "#     • Normalization layers (BatchNorm/LayerNorm): per-feature affine using data stats.\n",
    "#     • Pooling / Residual / Embedding / Graph message passing: not plain dense maps.\n",
    "#\n",
    "# Parameters θ are not necessarily just (W, b). They can include:\n",
    "#   - kernels/filters, projection matrices, normalization scales/shifts (γ, β),\n",
    "#     gating parameters, positional encodings, etc.\n",
    "#\n",
    "# Shape conventions still follow the same *input/output mapping* idea:\n",
    "#   - For a dense layer (vector input): x ∈ R^{B×d_in}  →  z ∈ R^{B×d_out}\n",
    "#   - For structured layers, the last dimension is typically \"features/channels\":\n",
    "#       * Conv2D: X ∈ R^{B×H×W×C_in}  →  Y ∈ R^{B×H'×W'×C_out}   (NHWC shown)\n",
    "#       * Self-attention: X ∈ R^{B×T×d_model} → Y ∈ R^{B×T×d_model}\n",
    "#       * LayerNorm over features: preserves shape; acts along the feature axis.\n",
    "#\n",
    "# Takeaway:\n",
    "#   - Dense = fully connected affine (W @ x + b) + optional σ.\n",
    "#   - Many layers use different/structured linear ops (or none), but you still track\n",
    "#     (batch, spatial/temporal dims?, features) → (same or new features) consistently.\n",
    "#\n",
    "# LOSS FUNCTIONS (what we optimize during training in *supervised* learning (ground truths are known))\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# PURPOSE\n",
    "# - A loss L(θ) is a scalar that measures how well the model f_θ fits data.\n",
    "# - Given a batch {(x_i, y_i)}_{i=1..B} (x_i ∈ R^{d_in} = input feauture and y_i ∈ R^{d_out} = ground-truth target vector) we minimize the per-batch empirical risk:\n",
    "#     L(θ) = (1/B) * Σ_i  ℓ( f_θ(x_i), y_i ) (ℓ = per-example loss)\n",
    "#\n",
    "# SHAPES (batch-first)\n",
    "# - Batch size: B\n",
    "# - Inputs:   X ∈ R^{B×d_in}\n",
    "# - Targets:  Y ∈ R^{B×d_out}  or  Y ∈ {0,…,K-1}^B (classification)\n",
    "# - Outputs:  Ŷ = f_θ(X) ∈ R^{B×d_out} or logits Z ∈ R^{B×K}\n",
    "#\n",
    "# REGRESSION LOSSES\n",
    "# -----------------------------------------------------------------------------\n",
    "#  - Ŷ, Y ∈ R^{B x d_out = 1 (scalar regression) or T (vector regression))}\n",
    "#  - Mean Squared Error (MSE):     ℓ(ŷ_i, y_i) =  1/d_out Σ_j (Ŷ[i,j] - Y[i,j])² (sum over output dims j)\n",
    "#  - Mean Absolute Error (MAE):    ℓ(ŷ_i, y_i) =  1/d_out Σ_j |Ŷ[i,j] - Y[i,j]|\n",
    "#  - Huber (smooth L1):            ℓ(ŷ_i, y_i) =  [1/d_out Σ_j huber_loss(Ŷ[i,j], Y[i,j])], where huber_loss(a, b) = 0.5 * (a - b)² if |a - b| < δ else δ * (|a - b| - 0.5 * δ)\n",
    "#  - L(θ) = 1/B Σ_i ℓ(Ŷ[i], Y[i]) (mean over batch)\n",
    "#\n",
    "# TRAINING LOOP (EPOCHS)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Dataset: D = {(x_i, y_i)}_{i=1..N}                       # N total samples\n",
    "# Stacked tensors: X ∈ R^{N×d_in},  Y ∈ R^{N×d_out}        # scalar regression ⇒ d_out = 1\n",
    "# Batch size B (e.g., 32/64/128):\n",
    "#   After shuffling each epoch, split D into M = ceil(N / B) batches.\n",
    "#   One batch: {(x_i, y_i)}_{i=1..B}  →  X_b ∈ R^{B×d_in}, Y_b ∈ R^{B×d_out}\n",
    "#   Note: the last batch may have < B samples unless you drop it.\n",
    "#\n",
    "# Iteration (a.k.a. Step, Update): ONE optimizer update using ONE batch (X_b, Y_b).\n",
    "#   1) Forward:   Ŷ_b = f_θ(X_b)                  # predictions for this batch\n",
    "#   2) Loss:      L   = loss_fn(Ŷ_b, Y_b)          # scalar batch loss\n",
    "#   3) Gradients: g   =  ∇_θ L                     # autodiff / backprop (same shape as θ)\n",
    "#   4) Update:    θ ← OPTIMIZER_STEP(θ, g)        # e.g., SGD / Adam / AdamW\n",
    "#\n",
    "# Epoch: one full pass over the dataset → exactly ONE iteration per batch.\n",
    "#   iterations_per_epoch = M = ceil(N / B)\n",
    "#\n",
    "# Typical training loop:\n",
    "#   for epoch in range(E):                      # E = num_epochs (hyperparameter to be chosen)\n",
    "#       shuffle(D)                              # randomize order\n",
    "#       split D into batches of size B.         # M = ceil(N / B) batches\n",
    "#       for (X_b, Y_b) in batches:              # M iterations this epoch\n",
    "#           do one iteration (forward → loss → grad → update) (updated theta passed to next iteration)\n",
    "#\n",
    "# --------------------------------- Practical Notes ---------------------------------\n",
    "# • Conceptually, the model is defined on a single example:\n",
    "#       f_θ : R^{d_in} → R^{d_out},   x ↦ f_θ(x)\n",
    "#\n",
    "#   In code, we usually *write* f_θ(x) as if x were a single input, but in the training loop we feed in whole batches X_b ∈ R^{B×d_in} \n",
    "#   The framework computes Ŷ_b = f_θ(X_b) by applying the same operations ndependently across the leading batch dimension B (broadcasting)\n",
    "#\n",
    "# • Scalar activation functions (ReLU, tanh, sigmoid, etc.) are elementwise:\n",
    "#       σ : R → R  (defined on scalars)\n",
    "#   but the same implementation works on batched tensors:\n",
    "#       z  ∈ R^{d_out}      → σ(z)  ∈ R^{d_out}\n",
    "#       Z  ∈ R^{B×d_out}    → σ(Z) ∈ R^{B×d_out}\n",
    "#   The framework just applies σ entrywise, including across the batch axis.\n",
    "#\n",
    "# • Conceptually, the loss starts as a per-example loss ℓ(ŷ, y) (e.g. MSE, cross-entropy). \n",
    "#\n",
    "#   In code we *define* loss_fn(Ŷ_b, Y_b, [optional weights]) as a function on an arbitrary batch (X_b, Y_b) by:\n",
    "#       – computing ℓ per example in the batch, then\n",
    "#       – aggregating them via a reduction (typically an unweighted or weighted\n",
    "#         mean over the batch).\n",
    "#\n",
    "# • Because everything is written in terms of array/tensor operations, the SAME\n",
    "#   f_θ and loss definitions work for any batch size B:\n",
    "#       B = 1      → single example\n",
    "#       B = N      → full-batch (one iteration per epoch)\n",
    "#       1 < B < N  → (mini-)batches\n",
    "#   Changing B only affects how we slice/form batches (X_b, Y_b, …), not the mathematical definitions of f_θ or the loss.\n",
    "#\n",
    "# - Using mean reductions in the loss (over outputs and over batch) keeps the loss scale independent of B; if you sum instead, adjust the learning rate.\n",
    "# - Gradient accumulation: to simulate a larger effective batch size k * B, accumulate (sum) grads over k batches and then apply one iteration update using the average gradient. \n",
    "# - Evaluations (train/val metrics) are typically computed at epoch boundaries without updating parameters.\n",
    "#\n",
    "# LOSS REDUCTIONS & WEIGHTING\n",
    "# -----------------------------------------------------------------------------\n",
    "# Default reductions\n",
    "# - Per-example loss:  ℓ_i = mean over output dims (keeps scale independent of d_out)\n",
    "# - Batch loss:        L_B = (1/B) * Σ_i ℓ_i(ŷ_i, y_i) \n",
    "# - Using MEANS (not sums) makes the loss insensitive to batch size B and output size d_out.\n",
    "#\n",
    "# Per-example weights (class imbalance, heteroskedastic noise, masks)\n",
    "# - General weighted mean over the batch (weighs each example i by w_i ≥ 0):\n",
    "#     L_B = [ Σ_i w_i * ℓ_i(ŷ_i, y_i) ] / [ Σ_i w_i ] \n",
    "#   • Unweighted mean: w_i = 1 for all i  → L_B = (1/B) Σ_i ℓ_i\n",
    "#   • Variable batch size / masked rows: set w_i=0 for masked examples; normalization by Σ w_i stabilizes scale\n",
    "#   • Inverse-variance weighting (Gaussian noise): w_i ∝ 1/σ_i^2\n",
    "#\n",
    "# Per-dimension / per-channel weights (multivariate targets)\n",
    "# - Define the per-example loss with dimension weights α_j ≥ 0\n",
    "#     ℓ_i = [ Σ_j α_j * loss_dim(ŷ_{ij}, y_{ij}) ] / [ Σ_j α_j ] (sum over output dims j)\n",
    "#   Then reduce across the batch as above (optionally with w_i). Normalizing by Σ_j α_j keeps scale stable if some dims are masked (α_j=0).\n",
    "#\n",
    "# L2 penalty (a.k.a. \"L2 regularization\") vs Weight Decay\n",
    "# - L2 penalty adds λ||θ||² directly to the loss:\n",
    "#     L_total = L_data + λ/2 Σ_k ||θ_k||²\n",
    "#   This changes the gradient to:  ∇_θ L_total = ∇_θ L_data + λ θ\n",
    "# - Weight decay shrinks parameters multiplicatively during the update step:\n",
    "#     θ ← (1 - ηλ) * θ - η * g , where g = ∇_θ L_data\n",
    "# - IMPORTANT EQUIVALENCE:\n",
    "#   • For PLAIN SGD, \"L2 penalty in the loss\" ≡ \"weight decay\" (they produce the same update).\n",
    "#   • For ADAPTIVE OPTIMIZERS (Adam/RMSProp), they are NOT equivalent. Prefer decoupled weight decay (AdamW) instead of adding λ/2||θ||² to the loss.\n",
    "# - Practical note: Typically DO NOT decay biases and normalization parameters (e.g., BatchNorm/LayerNorm scale & bias).\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "# COMMON OPTIMIZERS\n",
    "# -----------------------------------------------------------------------------\n",
    "# STOCHASTIC GRADIENT DESCENT (SGD)\n",
    "# - Update:  θ ← θ - η * g,  where g = ∇_θ L(θ; X_b, Y_b)\n",
    "# - Learning rate η: too large → divergent/oscillatory; too small → slow. (Typical 1e-4 to 1e-1.)\n",
    "# - Pros: simple, memory-light; strong generalization for large-scale vision when tuned.\n",
    "# - Cons: sensitive to scale/conditioning; may be slow in narrow valleys.\n",
    "#\n",
    "# MOMENTUM (SGD + momentum)\n",
    "# - Velocity: v ← μ v + g\n",
    "# - Update:   θ ← θ - η * v\n",
    "# - μ ∈ [0,1): smooths stochastic noise; accelerates along persistent directions. Nesterov evaluates the grad at a look-ahead point and often converges faster.\n",
    "#\n",
    "# ADAM (Adaptive Moment Estimation)\n",
    "# - Definitions:\n",
    "#     g_t = ∇_θ L_t(θ; X_b, Y_b)             # gradient at step/iteration t = 1, 2, ...\n",
    "#     m_0 = 0, v_0 = 0                       # initialize first/second moments to zero\n",
    "# - Exponential moving averages (per-parameter):\n",
    "#     m_t = β1 * m_{t-1} + (1-β1) * g_t     # EMA of gradients (momentum-like); more weight on recent grads\n",
    "#     v_t = β2 * v_{t-1} + (1-β2) * (g_t ⊙ g_t)  # EMA of squared grads (RMSProp-like)\n",
    "#   Unrolled (explicit) form for m_t:\n",
    "#     m_t = (1-β1) * Σ_{s=1..t} β1^{t-s} * g_s\n",
    "# - Bias correction (removes init bias toward zero at early t):\n",
    "#     m̂_t = m_t / (1 - β1^t),    v̂_t = v_t / (1 - β2^t)\n",
    "# - Elementwise adaptive step:\n",
    "#     step_t = m̂_t / (sqrt(v̂_t) + ε)\n",
    "#     θ ← θ - η * step_t\n",
    "# - Intuition:\n",
    "#   • m̂_t is a smoothed direction (reduces gradient noise).\n",
    "#   • v̂_t rescales by recent magnitude (dims with large variance get smaller steps).\n",
    "#   • ε (~1e-8) avoids division by zero; sets a small floor on the denominator.\n",
    "# - Typical defaults: β1=0.9, β2=0.999, ε=1e-8. AMSGrad uses v̂_t = max(v̂_t, v̂_{t-1}) for extra stability.\n",
    "#\n",
    "# ADAMW (Adam with decoupled weight decay)\n",
    "# - Same Adam moment updates (m_t, v_t, bias corrections).\n",
    "# - Decoupled decay: apply weight decay SEPARATELY from the gradient step:\n",
    "#     θ ← θ - η * [ m̂_t / (sqrt(v̂_t) + ε) ]   # Adam step\n",
    "#     θ ← θ - η * λ * θ                       # THEN decay (or equivalently θ ← (1 - ηλ) θ after the Adam step)\n",
    "# - Why prefer AdamW over Adam+L2-in-loss:\n",
    "#   • For adaptive methods, decoupled decay preserves the intended regularization behavior and avoids interactions with the per-parameter scaling.\n",
    "# - Good defaults: η=1e-3, weight_decay≈1e-4 (no decay on biases/normalization). Consider LR warmup for the first 100–1000 steps.\n",
    "#\n",
    "# PRACTICAL EXTRAS\n",
    "# - Gradient clipping: clip global norm (e.g., 1–5) to avoid rare exploding updates.\n",
    "# - LR schedules: warmup → cosine decay / step decay / ReduceLROnPlateau.\n",
    "# - Batch size ↔ LR: larger batches often allow slightly larger η; tune together.\n",
    "# - Init: He/Kaiming for ReLU/GELU; Xavier/Glorot for tanh/sigmoid.\n",
    "# - Regularization: dropout, data augmentation, label smoothing, early stopping on a validation split.\n",
    "# - Diagnostics:\n",
    "#   • If loss plateaus early → warmup + slightly higher η, or lower β2 (e.g., 0.99) for faster adaptation.\n",
    "#   • If training is noisy/unstable → lower η, increase B, add momentum/EMA of weights, or clip grads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9fb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This needs to be fleshed out more \n",
    "# ============================ How Gradients Are Actually Computed ============================ \n",
    "#\n",
    "# Context / Spaces\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# • Inputs:  x ∈ R^{d_in}\n",
    "# • Params:  θ = {θ_1,…,θ_L}  (collection of tensors; each grad has SAME SHAPE as its tensor)\n",
    "# • Outputs: ŷ = F(x; θ) ∈ R^{d_out}\n",
    "# • Per-example scalar loss: ψ : R^{d_out} × R^{d_out} → R,  ψ(ŷ, y)  (y is constant/inert)\n",
    "# • Scalar objective on (x, θ):\n",
    "#       Φ(x, θ) := ψ(F(x; θ), y)          # this is what we differentiate in practice\n",
    "#\n",
    "# Model as a composition of layers\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# z_0 = x\n",
    "# for ℓ = 1..L:\n",
    "#     z_ℓ = f_ℓ(z_{ℓ-1}; θ_ℓ)            # arbitrary differentiable op (dense/conv/attn/norm/etc.)\n",
    "# ŷ = z_L\n",
    "#\n",
    "# ======================================= REVERSE-MODE (BACKPROP / VJP) =========================================\n",
    "# Goal: compute ∂Φ/∂θ (and optionally ∂Φ/∂x) in ~one backward sweep (independent of #params per se).\n",
    "#\n",
    "# 1) Forward pass (cache primals/intermediates each layer needs for its VJP):\n",
    "#    cache_ℓ = (z_{ℓ-1}, z_ℓ, θ_ℓ, intermediates)\n",
    "#\n",
    "# 2) Seed at output (loss gradient wrt ŷ):\n",
    "#    λ := ∂ψ(ŷ, y)/∂ŷ = ∂Φ/∂ŷ ∈ R^{d_out}      # “backprop signal” injected at the model output\n",
    "#\n",
    "# 3) Reverse sweep (layer-local VJPs, i.e. chain rule through each f_ℓ):\n",
    "#    zbar_L := λ                                 # zbar_ℓ ≡ ∂Φ/∂z_ℓ\n",
    "#    for ℓ = L..1:\n",
    "#        # layer VJP evaluated at cache_ℓ\n",
    "#        # returns cotangents to inputs & params:\n",
    "#        zbar_{ℓ-1}, thetabar_ℓ = VJP_fℓ(cache_ℓ, zbar_ℓ)\n",
    "#        # thetabar_ℓ ≡ ∂Φ/∂θ_ℓ, same shape as θ_ℓ\n",
    "#        grad[θ_ℓ] += thetabar_ℓ\n",
    "#\n",
    "# 4) Results (gradients of the scalar objective Φ(x, θ) := ψ(F(x; θ), y)):\n",
    "#    ∂Φ/∂θ = {grad[θ_1], …, grad[θ_L]}\n",
    "#    (optional) ∂Φ/∂x = zbar_0\n",
    "#\n",
    "# Notes:\n",
    "# • No explicit Jacobians are formed; each primitive supplies a VJP rule.\n",
    "# • This is what PyTorch `loss.backward()` and JAX `grad` implement for scalar objectives Φ.\n",
    "#\n",
    "# ========================================= FORWARD-MODE (JVP) VIEW =============================================\n",
    "# Use when you want directional derivatives or input sensitivities and output dim is small\n",
    "# (or to build Hessian-vector products via JVP-of-VJP). It propagates tangents forward.\n",
    "#\n",
    "# Directional-derivative setup for the scalar objective Φ(x,θ) := ψ(F(x;θ), y):\n",
    "# • Pick tangents u_x (same shape as x) and u_θ = {u_{θ_ℓ}} (same shapes as θ_ℓ).\n",
    "# • We will compute dΦ(x,θ)[u_x, u_θ], the directional derivative of Φ along (u_x, u_θ).\n",
    "#\n",
    "# 1) Initialize tangents at input:\n",
    "#    v_0 := u_x                                # v_ℓ ≡ tangent for z_ℓ\n",
    "#\n",
    "# 2) Layerwise primal + tangent propagation (JVPs):\n",
    "#    for ℓ = 1..L:\n",
    "#        # Primal forward:\n",
    "#        z_ℓ = f_ℓ(z_{ℓ-1}; θ_ℓ)\n",
    "#        # Tangent update via local linearization:\n",
    "#        v_ℓ = J_z f_ℓ(z_{ℓ-1}; θ_ℓ) @ v_{ℓ-1}  +  J_θ f_ℓ(z_{ℓ-1}; θ_ℓ) @ u_{θ_ℓ}\n",
    "#\n",
    "# 3) Read out scalar directional derivative at the loss:\n",
    "#    λ := ∂ψ(ŷ, y)/∂ŷ = ∂Φ/∂ŷ               # same seed as reverse-mode\n",
    "#    dΦ(x,θ)[u_x, u_θ] = ⟨λ, v_L⟩             # inner product over output dims\n",
    "#\n",
    "# Consequences:\n",
    "# • To recover the FULL gradient ∂Φ/∂θ from forward-mode alone, you’d need one JVP per\n",
    "#   parameter direction (expensive). So forward-mode is great for *directional* queries,\n",
    "#   but reverse-mode is better for full gradients of scalar losses.\n",
    "#\n",
    "# ============================== DENSE-LAYER SPECIALIZATION (CLASSIC “DELTA” FORM) ===============================\n",
    "# Forward (primal):\n",
    "#   a_ℓ = W_ℓ z_{ℓ-1} + b_ℓ,  z_ℓ = σ_ℓ(a_ℓ)\n",
    "#\n",
    "# Forward (tangent):\n",
    "#   ȧ_ℓ = W_ℓ v_{ℓ-1} + (u_{W_ℓ} z_{ℓ-1}) + u_{b_ℓ}\n",
    "#   v_ℓ  = σ_ℓ′(a_ℓ) ⊙ ȧ_ℓ\n",
    "#\n",
    "# Reverse (backprop, i.e. gradients of Φ):\n",
    "#   δ_L = ∂Φ/∂z_L ⊙ σ_L′(a_L)               # (for softmax+CE with logits a_L: δ_L = softmax(a_L) − y)\n",
    "#   δ_ℓ = (W_{ℓ+1}^T δ_{ℓ+1}) ⊙ σ_ℓ′(a_ℓ)\n",
    "#   ∂Φ/∂W_ℓ = δ_ℓ (z_{ℓ-1})^T\n",
    "#   ∂Φ/∂b_ℓ = δ_ℓ\n",
    "#   ∂Φ/∂x   = W_1^T δ_1\n",
    "#\n",
    "# ============================================== BATCH CASE ======================================================\n",
    "# Batch {(x_i, y_i)}_{i=1..B}, optional per-example weights ω_i ≥ 0:\n",
    "#   L_B(θ) = [Σ_i ω_i ψ(F(x_i;θ), y_i)] / [Σ_i ω_i]   # scalar objective over the batch\n",
    "#\n",
    "# Reverse-mode (what frameworks do for batched losses):\n",
    "#   λ_i := ∂ψ(ŷ_i, y_i)/∂ŷ_i\n",
    "#   ∂L_B/∂θ = Σ_i (ω_i / Σ_j ω_j) · [ J_θ F(x_i,θ)^T · λ_i ]     # sum of tensors matching θ’s shapes\n",
    "#   (optional) ∂L_B/∂x_i = (ω_i / Σ_j ω_j) · [ J_x F(x_i,θ)^T · λ_i ]\n",
    "#\n",
    "# Forward-mode (directional batch derivative):\n",
    "#   Given per-example tangents u_{x_i} and a single u_θ:\n",
    "#     propagate (z_i, v_i) forward for each i to get v_{L,i},\n",
    "#     dL_B[(u_{x_1},…,u_{x_B}), u_θ] = (1/Σ_j ω_j) · Σ_i ω_i ⟨ λ_i , v_{L,i} ⟩\n",
    "#\n",
    "# =========================================== PRACTICAL SUMMARY ==================================================\n",
    "# • Training (scalar losses, many parameters): use REVERSE-MODE (backprop / VJP). That’s what\n",
    "#   PyTorch `loss.backward()` and JAX `grad` do: they give ∇_θ Φ with one backward sweep.\n",
    "# • Sensitivity analysis, Jacobian-vector or Hessian-vector products: use FORWARD-MODE JVP (and\n",
    "#   combos like JVP-of-VJP). It gives fast directional derivatives without materializing Jacobians/Hessians.\n",
    "# • Always use MEANS (not sums) across batch/output dims to keep loss scale stable vs B and d_out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd436ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CLASSIFIERS: BINARY CROSS-ENTROPY → MULTI-CLASS (BOLTZMANN / SOFTMAX VIEW)\n",
    "# ==============================================================================\n",
    "\n",
    "# 0) GENERAL CLASSIFIER SETUP\n",
    "# ----------------------------------------------------------------------\n",
    "# • Input features (one example):       x ∈ R^{d_in}\n",
    "# • Parameters:                         θ  (e.g., weights and biases of a linear or neural net)\n",
    "# • Classifier as logit-producing map:  f_θ : R^{d_in} → R^{d_out}\n",
    "#     – Output z = f_θ(x) are called logits: unnormalized scores (can be any real numbers).\n",
    "#     – Probabilities are obtained by passing z through a suitable nonlinearity:\n",
    "#         • Binary:    sigmoid/logistic\n",
    "#         • Multi-class: softmax\n",
    "#\n",
    "# For a batch of B examples:\n",
    "#   • Inputs:  X ∈ R^{B×d_in}\n",
    "#   • Logits:  Z = f_θ(X) ∈ R^{B×d_out}   (row i = logits for example i)\n",
    "\n",
    "\n",
    "# 1) BINARY CLASSIFICATION (2 CLASSES)\n",
    "# ----------------------------------------------------------------------\n",
    "# Example:  y ∈ {0,1}  (0 = negative, 1 = positive)\n",
    "#\n",
    "# MODEL (ONE-LOGIT FORMULATION)\n",
    "#   • For each example, produce a single real-valued logit:\n",
    "#         z = f_θ(x) ∈ R\n",
    "#\n",
    "#   • Interpret z as the log-odds of class 1 vs class 0:\n",
    "#         log( p(y=1|x)/p(y=0|x) ) = z\n",
    "#\n",
    "#   • Solve for the probabilities:\n",
    "#         p(y=1|x) = σ(z) = 1 / (1 + e^(−z)) ∈ (0,1)\n",
    "#         p(y=0|x) = 1 − σ(z)\n",
    "#\n",
    "#   For a batch:\n",
    "#         logits: z ∈ R^{B}           (or R^{B×1})\n",
    "#         labels: y ∈ {0,1}^{B}\n",
    "#\n",
    "#\n",
    "# 1.1) BINARY CROSS-ENTROPY LOSS\n",
    "# ----------------------------------------------------------------------\n",
    "# Cross-entropy measures how “far” the predicted Bernoulli distribution is\n",
    "# from the true Bernoulli label.\n",
    "#\n",
    "# For a single example (logit z, label y ∈ {0,1}):\n",
    "#\n",
    "#   • Predicted probability of class 1: p = σ(z)\n",
    "#\n",
    "#   • Binary cross-entropy (in terms of p):\n",
    "#         ℓ(z, y) = − [ y log p + (1 − y) log(1 − p) ]\n",
    "#                  = − [ y log σ(z) + (1 − y) log(1 − σ(z)) ]\n",
    "#\n",
    "# Numerically stable form (in terms of logits directly):\n",
    "#\n",
    "#   • softplus(t) = log(1 + e^t)\n",
    "#\n",
    "#   • Then:\n",
    "#         ℓ(z, y) = softplus(z) − y z\n",
    "#                  = log(1 + e^{z}) − y z\n",
    "#\n",
    "#   – These two expressions are mathematically equivalent:\n",
    "#         softplus(z) − y z  =  − [ y log σ(z) + (1 − y) log(1 − σ(z)) ]\n",
    "#\n",
    "# Batch loss (B examples):\n",
    "#         L(θ) = (1/B) Σ_{i=1}^B ℓ(z_i, y_i)\n",
    "#\n",
    "#\n",
    "# 1.2) BOLTZMANN / ENERGY INTERPRETATION (BINARY CASE)\n",
    "# ----------------------------------------------------------------------\n",
    "# Think of the classifier as an energy-based model with two states y ∈ {0,1}.\n",
    "#\n",
    "#   • Assign an energy to each label:\n",
    "#         E_θ(x, y)\n",
    "#\n",
    "#   • Boltzmann distribution at \"temperature\" T=1:\n",
    "#         p(y | x) = exp(−E_θ(x,y)) / Σ_{y'∈{0,1}} exp(−E_θ(x,y'))\n",
    "#\n",
    "# For binary classification, define the logit as an energy difference:\n",
    "#\n",
    "#   • Let:\n",
    "#         z(x) = log [ p(y=1|x) / p(y=0|x) ]\n",
    "#\n",
    "#   • From the Boltzmann form:\n",
    "#         p(y=1|x) = exp(−E_1) / (exp(−E_0) + exp(−E_1))\n",
    "#         p(y=0|x) = exp(−E_0) / (exp(−E_0) + exp(−E_1))\n",
    "#\n",
    "#     where E_0 = E_θ(x,0), E_1 = E_θ(x,1).\n",
    "#\n",
    "#   • Then:\n",
    "#         log [ p(y=1|x) / p(y=0|x) ]\n",
    "#       = log [ exp(−E_1) / exp(−E_0) ]\n",
    "#       = −E_1 + E_0\n",
    "#\n",
    "#   • Identify the logit with this energy difference:\n",
    "#         z(x) = E_0(x) − E_1(x)\n",
    "#\n",
    "#   • Substituting into the logistic formula:\n",
    "#         p(y=1|x) = 1 / (1 + e^{−z(x)})\n",
    "#\n",
    "# So the logistic/sigmoid output can be viewed as a special case of a 2-state\n",
    "# Boltzmann distribution where the logit encodes a relative energy difference.\n",
    "#\n",
    "#\n",
    "# 2) MULTI-CLASS CLASSIFICATION (K ≥ 3 CLASSES)\n",
    "# ----------------------------------------------------------------------\n",
    "# Example:   y ∈ {0, 1, ..., K−1}   (exactly one class per example)\n",
    "#\n",
    "# TARGETS\n",
    "#   • One example:\n",
    "#         y_int ∈ {0,...,K−1}     # integer class index\n",
    "#     or equivalently:\n",
    "#         y_onehot ∈ {0,1}^K with Σ_c y_onehot[c] = 1.\n",
    "#\n",
    "# MODEL OUTPUT\n",
    "#   • For each example, produce a K-dimensional logit vector:\n",
    "#         z = f_θ(x) ∈ R^{K}\n",
    "#\n",
    "#   • Softmax converts logits to a probability distribution over classes:\n",
    "#         p_c = exp(z_c) / Σ_j exp(z_j),    for c = 0,...,K−1\n",
    "#\n",
    "#   • Properties:\n",
    "#         p_c ∈ (0,1), Σ_c p_c = 1.\n",
    "#\n",
    "#   For a batch:\n",
    "#         logits: Z ∈ R^{B×K}   (row i = logits for example i)\n",
    "#         labels: y_int ∈ {0,...,K−1}^{B}\n",
    "#\n",
    "#\n",
    "# 2.1) CATEGORICAL CROSS-ENTROPY LOSS (MULTI-CLASS)\n",
    "# ----------------------------------------------------------------------\n",
    "# Cross-entropy between:\n",
    "#   – predicted distribution p(·|x_i) given by softmax(Z_i,·)\n",
    "#   – true distribution concentrated at y_int[i].\n",
    "#\n",
    "# For a single example (row Z_i ∈ R^{K}, label y_int[i]):\n",
    "#\n",
    "#   • With softmax written explicitly:\n",
    "#         p_c = exp(Z_{i,c}) / Σ_j exp(Z_{i,j})\n",
    "#\n",
    "#   • Loss:\n",
    "#         ℓ(Z_i, y_int[i]) = − log p_{y_int[i]}\n",
    "#                           = − log softmax(Z_i)[ y_int[i] ]\n",
    "#\n",
    "# Numerically stable form:\n",
    "#\n",
    "#   • logsumexp(Z_i) = log( Σ_j exp(Z_{i,j}) )\n",
    "#\n",
    "#   • Then:\n",
    "#         ℓ(Z_i, y_int[i]) = logsumexp(Z_i) − Z_{i, y_int[i]}\n",
    "#\n",
    "# Batch loss:\n",
    "#         L(θ) = (1/B) Σ_{i=1}^B ℓ(Z_i, y_int[i])\n",
    "#\n",
    "#\n",
    "# 2.2) BOLTZMANN / ENERGY INTERPRETATION (MULTI-CLASS, SOFTMAX)\n",
    "# ----------------------------------------------------------------------\n",
    "# Generalize the binary energy view to K classes y ∈ {0,...,K−1}.\n",
    "#\n",
    "#   • Assign an energy for each class:\n",
    "#         E_θ(x, c)    for c = 0,...,K−1\n",
    "#\n",
    "#   • Boltzmann distribution:\n",
    "#         p(c | x) = exp(−E_θ(x,c)) / Σ_j exp(−E_θ(x,j))\n",
    "#\n",
    "#   • If we define logits as negative energies:\n",
    "#         z_c(x) = −E_θ(x,c)\n",
    "#\n",
    "#     then the Boltzmann distribution becomes:\n",
    "#         p(c | x) = exp(z_c(x)) / Σ_j exp(z_j(x)) = softmax(z(x))_c\n",
    "#\n",
    "# So the standard softmax classifier is exactly a Boltzmann distribution over K\n",
    "# discrete states, with logits playing the role of negative energies.\n",
    "#\n",
    "#\n",
    "# 3) SHAPES / SUMMARY\n",
    "# ----------------------------------------------------------------------\n",
    "# BINARY (one-logit formulation):\n",
    "#   • logits:   z ∈ R^{B}                 (or R^{B×1})\n",
    "#   • labels:   y ∈ {0,1}^{B}\n",
    "#   • probs:    σ(z) ∈ (0,1)^{B}\n",
    "#   • loss:     L(θ) = (1/B) Σ_i [ softplus(z_i) − y_i z_i ]\n",
    "#\n",
    "# MULTI-CLASS (K classes):\n",
    "#   • logits:   Z ∈ R^{B×K}\n",
    "#   • labels:   y_int ∈ {0,...,K−1}^{B}\n",
    "#   • probs:    P ∈ [0,1]^{B×K}, row-wise softmax\n",
    "#   • loss:     L(θ) = (1/B) Σ_i [ logsumexp(Z_i) − Z_{i,y_int[i]} ]\n",
    "#\n",
    "# KEY TAKEAWAYS\n",
    "#   • Binary logistic regression: special case of a 2-state Boltzmann model with\n",
    "#     logistic (sigmoid) link.\n",
    "#   • Multi-class softmax: Boltzmann distribution over K states, with logits as\n",
    "#     negative energies.\n",
    "#   • Cross-entropy is the natural loss: it is the negative log-likelihood of the\n",
    "#     correct class under these probability models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f257bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# 4) CONVOLUTIONAL NEURAL NETWORKS (CNNs)\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Motivation:\n",
    "#   • Many data types have an intrinsic grid structure:\n",
    "#       – 1D: time series, audio waveforms\n",
    "#       – 2D: images (height × width), spectrograms\n",
    "#       – 3D: videos (time × height × width), volumetric data\n",
    "#   • In such data, nearby inputs are strongly correlated, and the same local\n",
    "#     pattern (edge, corner, texture) can appear anywhere in the grid.\n",
    "#   • Fully-connected layers ignore this structure (they mix all input coordinates\n",
    "#     with separate parameters), which is:\n",
    "#       – parameter-inefficient,\n",
    "#       – insensitive to translations unless explicitly learned,\n",
    "#       – prone to overfitting on large inputs (e.g., images).\n",
    "#\n",
    "# CNNs address this by:\n",
    "#   • Using local receptive fields (each neuron “sees” a small patch).\n",
    "#   • Sharing the same set of weights across spatial locations (convolution).\n",
    "#   • Producing feature maps that are translation-equivariant.\n",
    "#\n",
    "#\n",
    "# 4.1) BASIC OBJECTS IN A CNN\n",
    "# ----------------------------------------------------------------------------------\n",
    "# INPUT TENSOR (IMAGE EXAMPLE)\n",
    "#   • A color image can be thought of as:\n",
    "#         x ∈ R^{H×W×C_in}\n",
    "#     where:\n",
    "#         – H = height (pixels)\n",
    "#         – W = width  (pixels)\n",
    "#         – C_in = number of input channels (e.g., 3 for RGB)\n",
    "#   • For a batch of B images:\n",
    "#         X ∈ R^{B×H×W×C_in}    (or another convention like B×C_in×H×W)\n",
    "#\n",
    "# CONVOLUTIONAL FILTER / KERNEL\n",
    "#   • A convolution layer has a set of learnable filters (kernels).\n",
    "#   • Each filter is a small spatial window, e.g.:\n",
    "#         K ∈ R^{k_h×k_w×C_in}\n",
    "#     where k_h, k_w are the kernel’s height/width.\n",
    "#   • The layer typically has C_out such filters:\n",
    "#         – So all filters together form:\n",
    "#               W_conv ∈ R^{C_out×k_h×k_w×C_in}\n",
    "#         – Each filter produces one output channel (one feature map).\n",
    "#\n",
    "# CONVOLUTION OPERATION (CROSS-CORRELATION IN PRACTICE)\n",
    "#   • At each spatial position (i, j), we:\n",
    "#       – Take a local patch of the input x[i : i+k_h, j : j+k_w, :] ∈ R^{k_h×k_w×C_in}\n",
    "#       – Compute a dot product with the filter weights K:\n",
    "#             z[i, j, c_out] = Σ_{u,v,c_in} K_{c_out}[u,v,c_in] * x[i+u, j+v, c_in] + b_{c_out}\n",
    "#         where b_{c_out} is a bias for the output channel c_out.\n",
    "#   • Sliding the filter across all valid (i, j) positions yields a feature map\n",
    "#     z[:,:,c_out] ∈ R^{H'×W'}.\n",
    "#   • Stacking C_out such maps gives:\n",
    "#         z ∈ R^{H'×W'×C_out}\n",
    "#\n",
    "# HYPERPARAMETERS\n",
    "#   • Kernel size: (k_h, k_w) – size of the local patch.\n",
    "#   • Stride: how far the filter moves between positions (e.g., stride 1 vs stride 2).\n",
    "#   • Padding:\n",
    "#       – “valid”: no padding, output is smaller than input.\n",
    "#       – “same”: pad borders so output has similar spatial size as input.\n",
    "#\n",
    "#\n",
    "# 4.2) KEY PROPERTIES OF CONVOLUTION LAYERS\n",
    "# ----------------------------------------------------------------------------------\n",
    "# LOCAL RECEPTIVE FIELDS\n",
    "#   • Each output unit depends only on a local neighborhood in the input (determined\n",
    "#     by kernel size and depth of the network).\n",
    "#   • Deeper layers have larger “effective” receptive fields, combining information\n",
    "#     from larger regions of the original input.\n",
    "#\n",
    "# WEIGHT SHARING\n",
    "#   • The same filter (same set of weights) is applied at every spatial location.\n",
    "#   • This enforces:\n",
    "#       – Translation equivariance: if the input shifts, the feature map shifts\n",
    "#         similarly (ignoring boundary effects).\n",
    "#       – Parameter efficiency: number of parameters depends on kernel size and\n",
    "#         channel counts, not on the spatial size (H, W).\n",
    "#\n",
    "# FEATURE MAPS\n",
    "#   • Each output channel (feature map) corresponds to one learned pattern (filter).\n",
    "#   • Early layers often learn simple patterns (edges, color contrasts, textures).\n",
    "#   • Deeper layers combine these into more abstract features (shapes, object parts).\n",
    "#\n",
    "#\n",
    "# 4.3) NONLINEARITIES AND POOLING\n",
    "# ----------------------------------------------------------------------------------\n",
    "# NONLINEAR ACTIVATIONS\n",
    "#   • After each convolution, an elementwise nonlinearity is typically applied:\n",
    "#         h = σ(z)   (e.g., ReLU, GELU, etc.)\n",
    "#   • This makes the network a universal approximator, not just a stack of linear ops.\n",
    "#\n",
    "# POOLING (OPTIONAL)\n",
    "#   • Pooling reduces spatial resolution while preserving important information.\n",
    "#   • Common types:\n",
    "#       – Max pooling: take the maximum value over a small window (e.g., 2×2 region).\n",
    "#       – Average pooling: take the mean over the window.\n",
    "#   • Effects:\n",
    "#       – Adds some translation invariance (small shifts don’t change pooled output much).\n",
    "#       – Reduces H and W, thus reducing computation and number of subsequent parameters.\n",
    "#\n",
    "#\n",
    "# 4.4) CNN AS FEATURE EXTRACTOR + CLASSIFIER HEAD\n",
    "# ----------------------------------------------------------------------------------\n",
    "# VIEW AS TWO PARTS:\n",
    "#\n",
    "#   (1) Feature extractor (convolutional backbone):\n",
    "#       • Sequence of blocks:\n",
    "#             input → [conv → nonlinearity → (pool)] → ... → feature maps\n",
    "#       • After L layers, we obtain:\n",
    "#             F ∈ R^{H_L×W_L×C_L}\n",
    "#         where H_L, W_L are reduced spatial dimensions, C_L is the number of channels.\n",
    "#\n",
    "#   (2) Classifier head:\n",
    "#       • Convert feature maps into a vector:\n",
    "#             – Flatten: F → f ∈ R^{H_L·W_L·C_L}, or\n",
    "#             – Global pooling: e.g., average over spatial dimensions to get f ∈ R^{C_L}.\n",
    "#       • Feed f into a standard classifier (as in Sections 1–2):\n",
    "#             – Linear or MLP: f → logits z ∈ R^{K}\n",
    "#             – Probabilities: softmax(z) for K-class classification.\n",
    "#       • Loss: categorical cross-entropy (multi-class) or binary cross-entropy\n",
    "#         (if the task is binary or multi-label), exactly as previously described.\n",
    "#\n",
    "#\n",
    "# 4.5) SUMMARY: CNNs IN THE CLASSIFICATION PIPELINE\n",
    "# ----------------------------------------------------------------------------------\n",
    "#   • The classifier from Sections 1–2 assumes an input vector x ∈ R^{d_in}.\n",
    "#   • CNNs provide a way to map structured inputs (e.g. images) to such a vector:\n",
    "#         raw image → conv layers → feature maps → pooled / flattened feature vector f\n",
    "#         f → fully-connected classifier → logits → probabilities → cross-entropy loss\n",
    "#\n",
    "#   • Convolution layers:\n",
    "#       – exploit spatial locality,\n",
    "#       – reuse filters across the grid (weight sharing),\n",
    "#       – produce feature maps that respond to patterns (edges, shapes, textures),\n",
    "#       – feed these high-level representations into the same probabilistic framework\n",
    "#         (logits + softmax/sigmoid + cross-entropy) already developed for classifiers.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sci-dev)",
   "language": "python",
   "name": "sci-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
