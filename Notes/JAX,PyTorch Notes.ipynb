{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1595887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX x: [1. 2. 3.]  | shape: (3,)  | dtype: float32  | device: TFRT_CPU_0\n",
      "JAX M:\n",
      " [[ 0.  1.  2.  3.]\n",
      " [ 4.  5.  6.  7.]\n",
      " [ 8.  9. 10. 11.]] \n",
      "\n",
      "JAX jnp.sin(x): [0.84147096 0.9092974  0.14112   ]\n",
      "JAX M.T shape: (4, 3)\n",
      "JAX RNG x1: [-2.4424558  -2.0356805   0.20554423]\n",
      "JAX RNG x2 (same key): [-2.4424558  -2.0356805   0.20554423]\n",
      "JAX RNG x3 (different key): [ 1.2956359   1.3550105  -0.40960556] \n",
      "\n",
      "tensor([1., 2., 3.], device='mps:0')\n",
      "shape: torch.Size([3]) dtype: torch.float32 device: mps:0\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]], device='mps:0')\n",
      "shape: torch.Size([2, 3]) dtype: torch.float32 device: mps:0\n",
      "tensor([[-0.3413, -1.4359,  0.7669, -1.1818,  0.7512],\n",
      "        [-1.1156,  0.6561, -0.1908, -0.2969,  0.4658],\n",
      "        [ 0.2387, -0.0734, -1.1044,  0.8299,  0.7745],\n",
      "        [ 2.1827, -0.2245,  0.0699,  0.1540,  1.3947]], device='mps:0')\n",
      "shape: torch.Size([4, 5]) dtype: torch.float32 device: mps:0\n",
      "tensor([[0.1421, 0.2592, 0.4999, 0.1917, 0.5603],\n",
      "        [0.1563, 0.4288, 0.5015, 0.4667, 0.1219],\n",
      "        [0.5836, 0.6501, 0.0473, 0.4315, 0.3532],\n",
      "        [0.4890, 0.0302, 0.8794, 0.6769, 0.4517]], device='mps:0')\n",
      "shape: torch.Size([4, 5]) dtype: torch.float32 device: mps:0\n",
      "tensor([1., 1., 1., 1.], device='mps:0')\n",
      "shape: torch.Size([4]) dtype: torch.float32 device: mps:0\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]], device='mps:0')\n",
      "shape: torch.Size([4, 5]) dtype: torch.float32 device: mps:0\n"
     ]
    }
   ],
   "source": [
    "# ========================= Arrays/Tensors & RNGs: JAX vs PyTorch =========================\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import torch\n",
    "\n",
    "# --------------------------------- What is a JAX Array? (quick intuition) ---------------------------------\n",
    "# • A JAX array (jnp.ndarray) is JAX’s n-dimensional array type, closely mirroring NumPy arrays.\n",
    "# • You use jax.numpy (imported as jnp) instead of np:\n",
    "#     - jnp.array, jnp.zeros, jnp.ones, jnp.linspace, jnp.sin, jnp.dot, jnp.linalg.norm, etc.\n",
    "#     - Most of the time, jnp.* has the SAME signature and behavior as the corresponding np.* function.\n",
    "# • Differences vs plain NumPy:\n",
    "#     - JAX arrays live on accelerator devices (CPU/GPU/TPU) and are **immutable** (no in-place updates).\n",
    "#     - All jnp operations are traceable and differentiable (so grad/jit/vmap can see them).\n",
    "#     - Randomness is handled very differently (pure functional RNG, see RNG block below).\n",
    "# • Devices:\n",
    "#     - JAX automatically places arrays on a default device (CPU or GPU/TPU if configured).\n",
    "#     - You can check with `.device()` and move with jax.device_put if needed, but in practice you mostly ignore it.\n",
    "\n",
    "# Basic JAX array creation\n",
    "x_jax = jnp.array([1.0, 2.0, 3.0])          # (3,), float32\n",
    "M_jax = jnp.arange(12.0).reshape(3, 4)      # (3,4)\n",
    "print(\"JAX x:\", x_jax, \" | shape:\", x_jax.shape, \" | dtype:\", x_jax.dtype, \" | device:\", x_jax.device)\n",
    "print(\"JAX M:\\n\", M_jax, \"\\n\")\n",
    "\n",
    "# jnp behaves like NumPy:\n",
    "print(\"JAX jnp.sin(x):\", jnp.sin(x_jax))\n",
    "print(\"JAX M.T shape:\", M_jax.T.shape)\n",
    "\n",
    "# ---------------- RNGs in JAX vs NumPy ----------------\n",
    "# NumPy RNG (stateful, object/global-state API):\n",
    "# - Legacy: np.random.seed(...); draws mutate global state.\n",
    "# - Modern: rng = np.random.default_rng(seed); draws mutate rng’s INTERNAL STATE:\n",
    "#     rng = np.random.default_rng(0)\n",
    "#     x = rng.uniform(size=...)               # state ADVANCES implicitly\n",
    "#     y = rng.normal(size=...)                # next draw uses ADVANCED state, y differs from x\n",
    "# - You don’t pass the RNG to every call; state changes happen behind the scenes.\n",
    "# - There’s no built-in equivalent of “split” producing independent child generators per call (you can create\n",
    "#   new Generators with new seeds, or use bit_generator.jumped(), but it’s not as ergonomic as JAX’s split).\n",
    "# If rng is created with a fixed seed, the initial state is fixed, so the sequence of draws is reproducible across runs.\n",
    "# If rng is defined without specifying a seed, the initial state differs each call (uses OS entropy), so the sequence of draws differs each run.\n",
    "#\n",
    "# JAX RNG (functional, stateless API):\n",
    "# - You create an explicit PRNGKey and PASS IT IN to every random call.\n",
    "#     key = jax.random.key(seed)              # create a key (counter-based PRNG); note that a seed must be provided\n",
    "#     key, sub = jax.random.split(key)        # split into NEW independent keys\n",
    "#     x = jax.random.uniform(sub, shape, ...) # must provide a key\n",
    "# - Keys are PURE DATA. Reusing the SAME key → the SAME random numbers (no implicit advance)\n",
    "#      y = jax.random.uniform(sub, shape, ...) # y == x if reusing 'sub'\n",
    "#   You must SPLIT to advance the stream. This makes randomness reproducible under jit/vmap/pmap.\n",
    "# - Splitting is cheap and mathematically well-defined for counter-based PRNGs (e.g., Threefry/Philox).\n",
    "# - Best practices:\n",
    "#     • Never reuse a key; always do: key, k1 = jax.random.split(key)\n",
    "#     • For per-step randomness in training: key = jax.random.fold_in(key, step)  # mixes step into key\n",
    "#     • For batched randomness: keys = jax.random.split(key, B)  # one subkey per batch element (works with vmap)\n",
    "#\n",
    "# Why JAX does this:\n",
    "# - Pure functional RNG makes behavior deterministic under jit/vmap/pmap and easy to parallelize.\n",
    "# - You control exactly where/when randomness happens by threading keys through your program.\n",
    "#\n",
    "# In the linear regression example:\n",
    "#   key = jax.random.key(0)                       # initial key\n",
    "#   x = jax.random.uniform(key, (N, 1)..)         # BAD: reusing 'key' → same x on every call\n",
    "#   key, sub = jax.random.split(key)              # GOOD: advance RNG; 'key' 'sub' is a fresh stream every call\n",
    "#   x = jax.random.uniform(key, (N,1))            # draw with the fresh subkey\n",
    "#   noise = 0.05 * jax.random.normal(sub, (N,1))  # draw with the fresh subkey\n",
    "# Pattern: (key, k1, k2, ...) = jax.random.split(key, n) then use k1, k2,... for each random op.\n",
    "\n",
    "# (Note: in actual JAX code today you’d typically use jax.random.PRNGKey(seed), but the functional pattern above is the key idea.)\n",
    "\n",
    "# Tiny demo of JAX RNG behavior (functional keys)\n",
    "key = jax.random.PRNGKey(0)\n",
    "key, sub1, sub2 = jax.random.split(key, 3)\n",
    "x1 = jax.random.normal(sub1, (3,))\n",
    "x2 = jax.random.normal(sub1, (3,))   # same key → same values\n",
    "x3 = jax.random.normal(sub2, (3,))   # different key → different values\n",
    "print(\"JAX RNG x1:\", x1)\n",
    "print(\"JAX RNG x2 (same key):\", x2)\n",
    "print(\"JAX RNG x3 (different key):\", x3, \"\\n\")\n",
    "\n",
    "torch.manual_seed(0)  # reproducible per run (used for all random ops below but state is advanced as in NumPy)\n",
    "\n",
    "# --------------------------------- What is a Torch Tensor? (quick intuition) ---------------------------------\n",
    "# • A torch.Tensor is PyTorch’s fundamental n-dimensional array (like NumPy arrays, but with autograd + devices).\n",
    "# • Carries: shape (sizes along each axis), dtype (float32, int64, …), device (where it lives: 'cpu', 'cuda', 'mps'), and\n",
    "#   autograd metadata (requires_grad) for learning.\n",
    "# • Device matters: you can compute on CPU (default), NVIDIA GPU ('cuda', fastest), or Apple Silicon GPU ('mps', faster). Ops require all inputs\n",
    "#   on the *same* device; move with .to(\"cuda\") / .to(\"cpu\") / .to(\"mps\") or create directly on that device.\n",
    "# • Unlike plain Python lists, tensors support fast vectorized math, broadcasting, linear algebra (like NumPy arrays) and backprop.\n",
    "\n",
    "# -------------------------------------- TENSORS (fundamental object) --------------------------------------\n",
    "x0 = torch.tensor([1.0, 2.0, 3.0])                         # (3,), float32 CPU\n",
    "x1 = torch.zeros(2, 3)                                     # (2,3), float32 CPU\n",
    "x2 = torch.randn(4, 5)                                     # ~ N(0,1), (4,5), float32 CPU\n",
    "x3 = torch.rand(4, 5)                                      # ~ U(0,1), (4,5), float32 CPU\n",
    "x4 = torch.ones((4,))                                      # (4,) all ones, float32 CPU\n",
    "x_like = torch.zeros_like(x3)                              # shape & dtype like x3\n",
    "\n",
    "print(x0)\n",
    "print(\"shape:\", x0.shape, \"dtype:\", x0.dtype, \"device:\", x0.device)\n",
    "print(x1)\n",
    "print(\"shape:\", x1.shape, \"dtype:\", x1.dtype, \"device:\", x1.device)\n",
    "print(x2)\n",
    "print(\"shape:\", x2.shape, \"dtype:\", x2.dtype, \"device:\", x2.device)\n",
    "print(x3)\n",
    "print(\"shape:\", x3.shape, \"dtype:\", x3.dtype, \"device:\", x3.device)\n",
    "print(x4)\n",
    "print(\"shape:\", x4.shape, \"dtype:\", x4.dtype, \"device:\", x4.device)\n",
    "print(x_like)\n",
    "print(\"shape:\", x_like.shape, \"dtype:\", x_like.dtype, \"device:\", x_like.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd4147eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon GPU (MPS)\n",
      "A.device: mps:0\n",
      "x_dev: torch.Size([2, 2]) torch.float32 mps:0\n",
      "a after t_cpu += 1: [2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------ Device & dtype basics ------------------------------\n",
    "# Pick a device ONCE. By default PyTorch uses CPU; we switch to CUDA/MPS if available.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA GPU\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple Silicon GPU (MPS)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# (Optional) Make *future* factory-created tensors default to this device (PyTorch ≥ 2.0).\n",
    "# NOTE: torch.set_default_device(...) returns None; don't assign it to a variable.\n",
    "torch.set_default_device(device)\n",
    "\n",
    "# ------------------------------ Creation respects default device ------------------------------\n",
    "# Because we called torch.set_default_device(device), this lands on `device`.\n",
    "A = torch.arange(12., dtype=torch.float32).reshape(3, 4)\n",
    "print(\"A.device:\", A.device)   # -> e.g. mps/cuda/cpu depending on above\n",
    "\n",
    "# ------------------------------ Moving tensors across devices ------------------------------\n",
    "x64_cpu = torch.randn(2, 2, dtype=torch.float64, device=\"cpu\")  # force CPU for float64\n",
    "x_dev = x64_cpu.to(dtype=torch.float32, device=device)          # cast to float32 and move to device\n",
    "print(\"x_dev:\", x_dev.shape, x_dev.dtype, x_dev.device)\n",
    "\n",
    "# ------------------------------ NumPy interoperability (CPU only shares memory) ------------------------------\n",
    "a = np.array([1, 2, 3], dtype=np.float32)     # NumPy array (float32, usual default is float64)\n",
    "t_cpu = torch.from_numpy(a)                   # CPU tensor that SHARES MEMORY with `a`\n",
    "\n",
    "# If you need this tensor on GPU/MPS for compute:\n",
    "t_dev = t_cpu.to(device)                      # now on device (no longer shares with `a`)\n",
    "\n",
    "# Converting back to NumPy REQUIRES CPU:\n",
    "a2 = t_dev.cpu().numpy()                      # move to CPU first, then NumPy\n",
    "# `a2` now shares memory with the CPU tensor we just created; modifying that CPU tensor will reflect in `a2`.\n",
    "\n",
    "# Demonstrate in-place sharing on CPU:\n",
    "t_cpu += 1                                    # modifies both t_cpu and the original NumPy array `a`\n",
    "print(\"a after t_cpu += 1:\", a)               # shows the +1 update\n",
    "\n",
    "# TL;DR:\n",
    "# • Use `device = torch.device(...)` once; optionally call torch.set_default_device(device).\n",
    "# • On MPS, avoid float64: cast to float32/float16/bfloat16 when moving to \"mps\".\n",
    "# • .numpy() only works for CPU tensors; use `.cpu().numpy()` when coming from GPU/MPS.\n",
    "# • NumPy <-> Torch zero-copy sharing happens ONLY on CPU tensors (and same dtype/contiguous layout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e80fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ 1) Indexing / Reshape / Transpose ================\n",
      "\n",
      "--- JAX: indexing / reshape / transpose ---\n",
      "M_jax:\n",
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]]\n",
      "shape: (3, 4) \n",
      "\n",
      "row0_jax = M_jax[0]:\n",
      "[0 1 2 3]\n",
      "shape: (4,) \n",
      "\n",
      "col1_jax = M_jax[:, 1]:\n",
      "[1 5 9]\n",
      "shape: (3,) \n",
      "\n",
      "block_jax = M_jax[0:2, 1:4]:\n",
      "[[1 2 3]\n",
      " [5 6 7]]\n",
      "shape: (2, 3) \n",
      "\n",
      "JAX shapes: A: (2, 3, 4) A_T: (2, 4, 3) A_F: (2, 12) A_R: (6, 4)\n",
      "\n",
      "--- PyTorch: indexing / reshape / transpose ---\n",
      "M_t:\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "shape: torch.Size([3, 4]) \n",
      "\n",
      "row0_t = M_t[0]:\n",
      "tensor([0., 1., 2., 3.])\n",
      "shape: torch.Size([4]) \n",
      "\n",
      "col1_t = M_t[:, 1]:\n",
      "tensor([1., 5., 9.])\n",
      "shape: torch.Size([3]) \n",
      "\n",
      "block_t = M_t[0:2, 1:4]:\n",
      "tensor([[1., 2., 3.],\n",
      "        [5., 6., 7.]])\n",
      "shape: torch.Size([2, 3]) \n",
      "\n",
      "Torch shapes: A: torch.Size([2, 3, 4]) A_T: torch.Size([2, 4, 3]) A_F: torch.Size([2, 12]) A_R: torch.Size([6, 4])\n",
      "\n",
      "================ 1b) Unsqueeze / Expand Dims ================\n",
      "\n",
      "--- JAX: expand_dims / newaxis ---\n",
      "v_jax:\n",
      "[0 1 2 3]\n",
      "shape: (4,) \n",
      "\n",
      "v_row_jax = expand_dims(v_jax, axis=0):\n",
      "[[0 1 2 3]]\n",
      "shape: (1, 4) \n",
      "\n",
      "v_col_jax = expand_dims(v_jax, axis=1):\n",
      "[[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]]\n",
      "shape: (4, 1) \n",
      "\n",
      "v_row2_jax = v_jax[None, :]:\n",
      "[[0 1 2 3]]\n",
      "shape: (1, 4) \n",
      "\n",
      "v_col2_jax = v_jax[:, None]:\n",
      "[[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]]\n",
      "shape: (4, 1) \n",
      "\n",
      "B_jax (2,4):\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "shape: (2, 4) \n",
      "\n",
      "v_batched_jax = v_jax[None, :]:\n",
      "[[0 1 2 3]]\n",
      "shape: (1, 4) \n",
      "\n",
      "B_plus_v_jax = B_jax + v_batched_jax:\n",
      "[[1. 2. 3. 4.]\n",
      " [1. 2. 3. 4.]]\n",
      "shape: (2, 4) \n",
      "\n",
      "\n",
      "--- PyTorch: unsqueeze ---\n",
      "v_t:\n",
      "tensor([0., 1., 2., 3.])\n",
      "shape: torch.Size([4]) \n",
      "\n",
      "v_row_t = v_t.unsqueeze(0):\n",
      "tensor([[0., 1., 2., 3.]])\n",
      "shape: torch.Size([1, 4]) \n",
      "\n",
      "v_col_t = v_t.unsqueeze(1):\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "shape: torch.Size([4, 1]) \n",
      "\n",
      "v_row2_t = v_t[None, :]:\n",
      "tensor([[0., 1., 2., 3.]])\n",
      "shape: torch.Size([1, 4]) \n",
      "\n",
      "v_col2_t = v_t[:, None]:\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "shape: torch.Size([4, 1]) \n",
      "\n",
      "B_t (2,4):\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "shape: torch.Size([2, 4]) \n",
      "\n",
      "v_batched_t = v_t.unsqueeze(0):\n",
      "tensor([[0., 1., 2., 3.]])\n",
      "shape: torch.Size([1, 4]) \n",
      "\n",
      "v_expanded_t = v_batched_t.expand(2,4):\n",
      "tensor([[0., 1., 2., 3.],\n",
      "        [0., 1., 2., 3.]])\n",
      "shape: torch.Size([2, 4]) \n",
      "\n",
      "B_plus_v_t = B_t + v_expanded_t:\n",
      "tensor([[1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.]])\n",
      "shape: torch.Size([2, 4]) \n",
      "\n",
      "\n",
      "================ 2) Broadcasting & Elementwise Ops ================\n",
      "\n",
      "--- JAX: broadcasting & elementwise ---\n",
      "B_jax (3,1):\n",
      "[[0. ]\n",
      " [0.5]\n",
      " [1. ]]\n",
      "shape: (3, 1) \n",
      "\n",
      "C_jax (1,4):\n",
      "[[-1.         -0.33333328  0.33333337  1.        ]]\n",
      "shape: (1, 4) \n",
      "\n",
      "B_jax + C_jax (3,4):\n",
      "[[-1.         -0.33333328  0.33333337  1.        ]\n",
      " [-0.5         0.16666672  0.8333334   1.5       ]\n",
      " [ 0.          0.66666675  1.3333334   2.        ]]\n",
      "shape: (3, 4) \n",
      "\n",
      "JAX E: [3.0907025  0.15852904 0.         1.841471   4.9092975 ] | mean(E): 2.0\n",
      "\n",
      "--- PyTorch: broadcasting & elementwise ---\n",
      "B_t (3,1):\n",
      "tensor([[0.0000],\n",
      "        [0.5000],\n",
      "        [1.0000]])\n",
      "shape: torch.Size([3, 1]) \n",
      "\n",
      "C_t (1,4):\n",
      "tensor([[-1.0000, -0.3333,  0.3333,  1.0000]])\n",
      "shape: torch.Size([1, 4]) \n",
      "\n",
      "B_t + C_t (3,4):\n",
      "tensor([[-1.0000, -0.3333,  0.3333,  1.0000],\n",
      "        [-0.5000,  0.1667,  0.8333,  1.5000],\n",
      "        [ 0.0000,  0.6667,  1.3333,  2.0000]])\n",
      "shape: torch.Size([3, 4]) \n",
      "\n",
      "Torch E: tensor([3.0907, 0.1585, 0.0000, 1.8415, 4.9093]) | mean(E): 2.0\n",
      "\n",
      "================ 3) Stacking vs Concatenation ================\n",
      "\n",
      "--- JAX: jnp.stack vs jnp.concatenate ---\n",
      "stack0_jax (axis=0):\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "shape: (2, 3) \n",
      "\n",
      "stack1_jax (axis=1):\n",
      "[[1. 4.]\n",
      " [2. 5.]\n",
      " [3. 6.]]\n",
      "shape: (3, 2) \n",
      "\n",
      "cat0_jax (axis=0):\n",
      "[1. 2. 3. 4. 5. 6.]\n",
      "shape: (6,) \n",
      "\n",
      "JAX S0,S1,S2 shapes: (2, 2, 3) (2, 2, 3) (2, 3, 2)\n",
      "JAX C_row.shape: (4, 3) C_col.shape: (2, 6)\n",
      "\n",
      "--- PyTorch: torch.stack vs torch.cat ---\n",
      "stack0_t (dim=0):\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "shape: torch.Size([2, 3]) \n",
      "\n",
      "stack1_t (dim=1):\n",
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n",
      "shape: torch.Size([3, 2]) \n",
      "\n",
      "cat0_t (dim=0):\n",
      "tensor([1., 2., 3., 4., 5., 6.])\n",
      "shape: torch.Size([6]) \n",
      "\n",
      "Torch S0,S1,S2 shapes: torch.Size([2, 2, 3]) torch.Size([2, 2, 3]) torch.Size([2, 3, 2])\n",
      "Torch C_row.shape: torch.Size([4, 3]) C_col.shape: torch.Size([2, 6])\n",
      "\n",
      "================ 4) Linear Algebra / Matmul ================\n",
      "\n",
      "--- JAX: matmul / linalg ---\n",
      "Y_jax = X @ W^T:\n",
      "[[ 0.3        0.8        1.3      ]\n",
      " [ 0.8        2.55       4.2999997]\n",
      " [ 1.3        4.2999997  7.3      ]\n",
      " [ 1.8        6.05      10.3      ]\n",
      " [ 2.3000002  7.7999997 13.3      ]\n",
      " [ 2.8        9.549999  16.3      ]\n",
      " [ 3.3000002 11.3       19.3      ]]\n",
      "shape: (7, 3) \n",
      "\n",
      "JAX ||W||_F: 3.185906410217285\n",
      "\n",
      "--- PyTorch: matmul / linalg ---\n",
      "Y_t3 = X @ W^T:\n",
      "tensor([[ 0.3000,  0.8000,  1.3000],\n",
      "        [ 0.8000,  2.5500,  4.3000],\n",
      "        [ 1.3000,  4.3000,  7.3000],\n",
      "        [ 1.8000,  6.0500, 10.3000],\n",
      "        [ 2.3000,  7.8000, 13.3000],\n",
      "        [ 2.8000,  9.5500, 16.3000],\n",
      "        [ 3.3000, 11.3000, 19.3000]])\n",
      "shape: torch.Size([7, 3]) \n",
      "\n",
      "Torch ||W||_F: 3.185906410217285\n",
      "Q_t (from QR):\n",
      "tensor([[ 0.0000e+00, -7.7460e-01,  1.9160e-01,  6.0182e-01, -3.3214e-02],\n",
      "        [-1.8257e-01, -5.1640e-01, -3.1815e-01, -5.3237e-01,  5.6157e-01],\n",
      "        [-3.6515e-01, -2.5820e-01, -3.0453e-01, -2.7916e-01, -7.9331e-01],\n",
      "        [-5.4772e-01,  5.1035e-07,  7.9710e-01, -2.5185e-01,  3.4751e-02],\n",
      "        [-7.3030e-01,  2.5820e-01, -3.6602e-01,  4.6156e-01,  2.3020e-01]])\n",
      "shape: torch.Size([5, 5]) \n",
      "\n",
      "R_t (from QR):\n",
      "tensor([[-2.7386e+01, -2.9212e+01, -3.1038e+01, -3.2863e+01, -3.4689e+01],\n",
      "        [ 0.0000e+00, -1.2910e+00, -2.5820e+00, -3.8730e+00, -5.1640e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  8.3988e-07,  1.2814e-06,  1.9666e-06],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0472e-06, -1.8734e-06],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.3199e-07]])\n",
      "shape: torch.Size([5, 5]) \n",
      "\n",
      "eigvals_t:\n",
      "tensor([ 6.3912e+01+0.0000e+00j, -3.9117e+00+0.0000e+00j,\n",
      "        -5.9613e-07+6.4725e-07j, -5.9613e-07-6.4725e-07j,\n",
      "        -3.5704e-08+0.0000e+00j])\n",
      "shape: torch.Size([5]) \n",
      "\n",
      "\n",
      "================ 5) Einsum Patterns ================\n",
      "\n",
      "--- JAX: einsum ---\n",
      "JAX trace = einsum('ii->', M):\n",
      "30.0\n",
      "shape: () \n",
      "\n",
      "JAX diag = einsum('ii->i', M):\n",
      "[ 0.  5. 10. 15.]\n",
      "shape: (4,) \n",
      "\n",
      "JAX outer = einsum('i,j->ij', x, y):\n",
      "[[-5.  0.  5. 10.]\n",
      " [-6.  0.  6. 12.]\n",
      " [-7.  0.  7. 14.]\n",
      " [-8.  0.  8. 16.]\n",
      " [-9.  0.  9. 18.]]\n",
      "shape: (5, 4) \n",
      "\n",
      "JAX batched_mm = einsum('bij,bjk->bik', As, Bs):\n",
      "[[[ 40.  45.  50.  55.]\n",
      "  [ 40.  45.  50.  55.]]\n",
      "\n",
      " [[140. 145. 150. 155.]\n",
      "  [140. 145. 150. 155.]]\n",
      "\n",
      " [[240. 245. 250. 255.]\n",
      "  [240. 245. 250. 255.]]]\n",
      "shape: (3, 2, 4) \n",
      "\n",
      "JAX yb = einsum('bi,ij,bj->b', xb, A2, xb):\n",
      "[ 30. 255. 730.]\n",
      "shape: (3,) \n",
      "\n",
      "\n",
      "--- PyTorch: einsum ---\n",
      "Torch trace = einsum('ii->', M):\n",
      "tensor(30.)\n",
      "shape: torch.Size([]) \n",
      "\n",
      "Torch diag = einsum('ii->i', M):\n",
      "tensor([ 0.,  5., 10., 15.])\n",
      "shape: torch.Size([4]) \n",
      "\n",
      "Torch outer = einsum('i,j->ij', x, y):\n",
      "tensor([[-5.,  0.,  5., 10.],\n",
      "        [-6.,  0.,  6., 12.],\n",
      "        [-7.,  0.,  7., 14.],\n",
      "        [-8.,  0.,  8., 16.],\n",
      "        [-9.,  0.,  9., 18.]])\n",
      "shape: torch.Size([5, 4]) \n",
      "\n",
      "Torch batched_mm = einsum('bij,bjk->bik', As, Bs):\n",
      "tensor([[[ 40.,  45.,  50.,  55.],\n",
      "         [ 40.,  45.,  50.,  55.]],\n",
      "\n",
      "        [[140., 145., 150., 155.],\n",
      "         [140., 145., 150., 155.]],\n",
      "\n",
      "        [[240., 245., 250., 255.],\n",
      "         [240., 245., 250., 255.]]])\n",
      "shape: torch.Size([3, 2, 4]) \n",
      "\n",
      "Torch yb = einsum('bi,ij,bj->b', xb, A2, xb):\n",
      "tensor([ 30., 255., 730.])\n",
      "shape: torch.Size([3]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#========================= Core Tensor Operations: JAX vs PyTorch =========================\n",
    "# Indexing/slicing, reshaping, broadcasting, stacking/concat, linear algebra, einsum.\n",
    "\n",
    "# Reuse a simple helper\n",
    "def show(name, arr):\n",
    "    print(f\"{name}:\")\n",
    "    print(arr)\n",
    "    try:\n",
    "        shape = arr.shape\n",
    "    except AttributeError:\n",
    "        shape = \"no .shape\"\n",
    "    print(\"shape:\", shape, \"\\n\")\n",
    "\n",
    "torch.set_default_device('cpu') # reset to CPU for this section\n",
    "\n",
    "# =========================================================================================\n",
    "# 1a) INDEXING, SLICING, RESHAPING, TRANSPOSING\n",
    "# =========================================================================================\n",
    "\n",
    "print(\"\\n================ 1) Indexing / Reshape / Transpose ================\")\n",
    "\n",
    "# ------------------------------ JAX ------------------------------\n",
    "print(\"\\n--- JAX: indexing / reshape / transpose ---\")\n",
    "\n",
    "M_jax = jnp.arange(12).reshape(3, 4)   # [[0,1,2,3],\n",
    "                                       #  [4,5,6,7],\n",
    "                                       #  [8,9,10,11]]\n",
    "\n",
    "row0_jax  = M_jax[0]           # (4,)\n",
    "col1_jax  = M_jax[:, 1]        # (3,)\n",
    "block_jax = M_jax[0:2, 1:4]    # (2,3)\n",
    "\n",
    "show(\"M_jax\", M_jax)\n",
    "show(\"row0_jax = M_jax[0]\", row0_jax)\n",
    "show(\"col1_jax = M_jax[:, 1]\", col1_jax)\n",
    "show(\"block_jax = M_jax[0:2, 1:4]\", block_jax)\n",
    "\n",
    "# Reshape / transpose\n",
    "A_jax   = jnp.arange(24).reshape(2, 3, 4)       # (2,3,4)\n",
    "A_T_jax = jnp.transpose(A_jax, (0, 2, 1))       # (2,4,3)\n",
    "A_F_jax = jnp.reshape(A_jax, (2, -1))           # (2,12)\n",
    "A_R_jax = A_jax.reshape(6, 4)                   # (6,4)\n",
    "\n",
    "print(\"JAX shapes: A:\", A_jax.shape,\n",
    "      \"A_T:\", A_T_jax.shape,\n",
    "      \"A_F:\", A_F_jax.shape,\n",
    "      \"A_R:\", A_R_jax.shape)\n",
    "\n",
    "# ------------------------------ PyTorch ------------------------------\n",
    "print(\"\\n--- PyTorch: indexing / reshape / transpose ---\")\n",
    "\n",
    "M_t = torch.arange(12., dtype=torch.float32).reshape(3, 4)\n",
    "\n",
    "row0_t  = M_t[0]           # (4,)\n",
    "col1_t  = M_t[:, 1]        # (3,)\n",
    "block_t = M_t[0:2, 1:4]    # (2,3)\n",
    "\n",
    "show(\"M_t\", M_t)\n",
    "show(\"row0_t = M_t[0]\", row0_t)\n",
    "show(\"col1_t = M_t[:, 1]\", col1_t)\n",
    "show(\"block_t = M_t[0:2, 1:4]\", block_t)\n",
    "\n",
    "A_t   = torch.arange(24.).reshape(2, 3, 4)   # (2,3,4)\n",
    "A_T_t = A_t.transpose(1, 2)                  # (2,4,3) (swap dims 1 and 2)\n",
    "A_F_t = A_t.flatten(start_dim=1)             # (2,12)\n",
    "A_R_t = A_t.reshape(6, 4)                    # (6,4)\n",
    "\n",
    "print(\"Torch shapes: A:\", A_t.shape,\n",
    "      \"A_T:\", A_T_t.shape,\n",
    "      \"A_F:\", A_F_t.shape,\n",
    "      \"A_R:\", A_R_t.shape)\n",
    "\n",
    "# =========================================================================================\n",
    "# 1b) UNSQUEEZING/EXPANDING DIMS\n",
    "# =========================================================================================\n",
    "\n",
    "print(\"\\n================ 1b) Unsqueeze / Expand Dims ================\")\n",
    "\n",
    "# ------------------------------ JAX ------------------------------\n",
    "print(\"\\n--- JAX: expand_dims / newaxis ---\")\n",
    "\n",
    "v_jax = jnp.arange(4)   # (4,)\n",
    "show(\"v_jax\", v_jax)\n",
    "\n",
    "# Row vs column views\n",
    "v_row_jax = jnp.expand_dims(v_jax, axis=0)   # (1,4)\n",
    "v_col_jax = jnp.expand_dims(v_jax, axis=1)   # (4,1)\n",
    "\n",
    "# Same thing using None / newaxis\n",
    "v_row2_jax = v_jax[None, :]   # (1,4)\n",
    "v_col2_jax = v_jax[:, None]   # (4,1)\n",
    "\n",
    "show(\"v_row_jax = expand_dims(v_jax, axis=0)\", v_row_jax)\n",
    "show(\"v_col_jax = expand_dims(v_jax, axis=1)\", v_col_jax)\n",
    "show(\"v_row2_jax = v_jax[None, :]\", v_row2_jax)\n",
    "show(\"v_col2_jax = v_jax[:, None]\", v_col2_jax)\n",
    "\n",
    "# Example: broadcasting with an extra batch dim\n",
    "B_jax = jnp.ones((2, 4))             # (2,4)\n",
    "v_batched_jax = v_jax[None, :]       # (1,4) → broadcast to (2,4) when added\n",
    "B_plus_v_jax = B_jax + v_batched_jax\n",
    "show(\"B_jax (2,4)\", B_jax)\n",
    "show(\"v_batched_jax = v_jax[None, :]\", v_batched_jax)\n",
    "show(\"B_plus_v_jax = B_jax + v_batched_jax\", B_plus_v_jax)\n",
    "\n",
    "# ------------------------------ PyTorch ------------------------------\n",
    "print(\"\\n--- PyTorch: unsqueeze ---\")\n",
    "\n",
    "v_t = torch.arange(4., dtype=torch.float32)   # (4,)\n",
    "show(\"v_t\", v_t)\n",
    "\n",
    "# Row vs column views\n",
    "v_row_t = v_t.unsqueeze(0)   # (1,4)\n",
    "v_col_t = v_t.unsqueeze(1)   # (4,1)\n",
    "\n",
    "# Same idea with indexing\n",
    "v_row2_t = v_t[None, :]      # (1,4)\n",
    "v_col2_t = v_t[:, None]      # (4,1)\n",
    "\n",
    "show(\"v_row_t = v_t.unsqueeze(0)\", v_row_t)\n",
    "show(\"v_col_t = v_t.unsqueeze(1)\", v_col_t)\n",
    "show(\"v_row2_t = v_t[None, :]\", v_row2_t)\n",
    "show(\"v_col2_t = v_t[:, None]\", v_col2_t)\n",
    "\n",
    "# Example: expand to a batch\n",
    "B_t = torch.ones(2, 4)                # (2,4)\n",
    "v_batched_t = v_t.unsqueeze(0)        # (1,4)\n",
    "v_expanded_t = v_batched_t.expand(2, 4)  # (2,4) (no data copy)\n",
    "B_plus_v_t = B_t + v_expanded_t\n",
    "\n",
    "show(\"B_t (2,4)\", B_t)\n",
    "show(\"v_batched_t = v_t.unsqueeze(0)\", v_batched_t)\n",
    "show(\"v_expanded_t = v_batched_t.expand(2,4)\", v_expanded_t)\n",
    "show(\"B_plus_v_t = B_t + v_expanded_t\", B_plus_v_t)\n",
    "\n",
    "\n",
    "# =========================================================================================\n",
    "# 2) BROADCASTING & ELEMENTWISE OPERATIONS\n",
    "# =========================================================================================\n",
    "\n",
    "print(\"\\n================ 2) Broadcasting & Elementwise Ops ================\")\n",
    "\n",
    "# ------------------------------ JAX ------------------------------\n",
    "print(\"\\n--- JAX: broadcasting & elementwise ---\")\n",
    "\n",
    "B_jax = jnp.linspace(0.0, 1.0, 3).reshape(3, 1)   # (3,1)\n",
    "C_jax = jnp.linspace(-1.0, 1.0, 4).reshape(1, 4)  # (1,4)\n",
    "BC_jax = B_jax + C_jax                            # (3,4) via broadcasting\n",
    "\n",
    "show(\"B_jax (3,1)\", B_jax)\n",
    "show(\"C_jax (1,4)\", C_jax)\n",
    "show(\"B_jax + C_jax (3,4)\", BC_jax)\n",
    "\n",
    "Z_jax = jnp.linspace(-2.0, 2.0, 5)\n",
    "E_jax = Z_jax**2 + jnp.sin(Z_jax)\n",
    "print(\"JAX E:\", E_jax, \"| mean(E):\", float(jnp.mean(E_jax)))\n",
    "\n",
    "# ------------------------------ PyTorch ------------------------------\n",
    "print(\"\\n--- PyTorch: broadcasting & elementwise ---\")\n",
    "\n",
    "B_t = torch.linspace(0.0, 1.0, steps=3).reshape(3, 1)   # (3,1)\n",
    "C_t = torch.linspace(-1.0, 1.0, steps=4).reshape(1, 4)  # (1,4)\n",
    "BC_t = B_t + C_t                                        # (3,4)\n",
    "\n",
    "show(\"B_t (3,1)\", B_t)\n",
    "show(\"C_t (1,4)\", C_t)\n",
    "show(\"B_t + C_t (3,4)\", BC_t)\n",
    "\n",
    "Z_t = torch.linspace(-2.0, 2.0, steps=5)\n",
    "E_t = Z_t**2 + torch.sin(Z_t)\n",
    "print(\"Torch E:\", E_t, \"| mean(E):\", E_t.mean().item())\n",
    "\n",
    "# =========================================================================================\n",
    "# 3) STACKING vs CONCATENATION\n",
    "# =========================================================================================\n",
    "\n",
    "print(\"\\n================ 3) Stacking vs Concatenation ================\")\n",
    "\n",
    "# ------------------------------ JAX ------------------------------\n",
    "print(\"\\n--- JAX: jnp.stack vs jnp.concatenate ---\")\n",
    "\n",
    "v1_jax = jnp.array([1., 2., 3.])\n",
    "v2_jax = jnp.array([4., 5., 6.])\n",
    "\n",
    "# stack: add a NEW axis\n",
    "stack0_jax = jnp.stack([v1_jax, v2_jax], axis=0)   # (2,3)\n",
    "stack1_jax = jnp.stack([v1_jax, v2_jax], axis=1)   # (3,2)\n",
    "\n",
    "# concat: join along existing axis\n",
    "cat0_jax = jnp.concatenate([v1_jax, v2_jax], axis=0)  # (6,)\n",
    "\n",
    "show(\"stack0_jax (axis=0)\", stack0_jax)\n",
    "show(\"stack1_jax (axis=1)\", stack1_jax)\n",
    "show(\"cat0_jax (axis=0)\", cat0_jax)\n",
    "\n",
    "A_jax2 = jnp.arange(6).reshape(2, 3)\n",
    "B_jax2 = A_jax2 + 10\n",
    "\n",
    "S0_jax = jnp.stack([A_jax2, B_jax2], axis=0)  # (2,2,3)\n",
    "S1_jax = jnp.stack([A_jax2, B_jax2], axis=1)  # (2,2,3)\n",
    "S2_jax = jnp.stack([A_jax2, B_jax2], axis=2)  # (2,3,2)\n",
    "\n",
    "print(\"JAX S0,S1,S2 shapes:\", S0_jax.shape, S1_jax.shape, S2_jax.shape)\n",
    "\n",
    "C_row_jax = jnp.concatenate([A_jax2, B_jax2], axis=0)  # (4,3)\n",
    "C_col_jax = jnp.concatenate([A_jax2, B_jax2], axis=1)  # (2,6)\n",
    "print(\"JAX C_row.shape:\", C_row_jax.shape, \"C_col.shape:\", C_col_jax.shape)\n",
    "\n",
    "# ------------------------------ PyTorch ------------------------------\n",
    "print(\"\\n--- PyTorch: torch.stack vs torch.cat ---\")\n",
    "\n",
    "v1_t = torch.tensor([1., 2., 3.])\n",
    "v2_t = torch.tensor([4., 5., 6.])\n",
    "\n",
    "stack0_t = torch.stack([v1_t, v2_t], dim=0)  # (2,3)\n",
    "stack1_t = torch.stack([v1_t, v2_t], dim=1)  # (3,2)\n",
    "cat0_t   = torch.cat([v1_t, v2_t], dim=0)    # (6,)\n",
    "\n",
    "show(\"stack0_t (dim=0)\", stack0_t)\n",
    "show(\"stack1_t (dim=1)\", stack1_t)\n",
    "show(\"cat0_t (dim=0)\", cat0_t)\n",
    "\n",
    "A_t2 = torch.arange(6.).reshape(2, 3)\n",
    "B_t2 = A_t2 + 10\n",
    "\n",
    "S0_t = torch.stack([A_t2, B_t2], dim=0)  # (2,2,3)\n",
    "S1_t = torch.stack([A_t2, B_t2], dim=1)  # (2,2,3)\n",
    "S2_t = torch.stack([A_t2, B_t2], dim=2)  # (2,3,2)\n",
    "\n",
    "print(\"Torch S0,S1,S2 shapes:\", S0_t.shape, S1_t.shape, S2_t.shape)\n",
    "\n",
    "C_row_t = torch.cat([A_t2, B_t2], dim=0)  # (4,3)\n",
    "C_col_t = torch.cat([A_t2, B_t2], dim=1)  # (2,6)\n",
    "print(\"Torch C_row.shape:\", C_row_t.shape, \"C_col.shape:\", C_col_t.shape)\n",
    "\n",
    "# =========================================================================================\n",
    "# 4) LINEAR ALGEBRA: MATMUL, NORMS, QR, EIG\n",
    "# =========================================================================================\n",
    "\n",
    "print(\"\\n================ 4) Linear Algebra / Matmul ================\")\n",
    "\n",
    "# ------------------------------ JAX ------------------------------\n",
    "print(\"\\n--- JAX: matmul / linalg ---\")\n",
    "\n",
    "X_jax = jnp.arange(35., dtype=jnp.float32).reshape(7, 5) / 10.0  # (7,5)\n",
    "W_jax = jnp.arange(15., dtype=jnp.float32).reshape(3, 5) / 10.0  # (3,5)\n",
    "\n",
    "Y_jax = X_jax @ W_jax.T     # (7,3)\n",
    "show(\"Y_jax = X @ W^T\", Y_jax)\n",
    "\n",
    "normW_jax = jnp.linalg.norm(W_jax)  # Frobenius norm\n",
    "print(\"JAX ||W||_F:\", float(normW_jax))\n",
    "\n",
    "M_square_jax = jnp.arange(25., dtype=jnp.float32).reshape(5, 5)\n",
    "\n",
    "# NOTE: kernel crashes here; jnp linalg issues? \n",
    "# Q_jax, R_jax = jnp.linalg.qr(M_square_jax)       # QR\n",
    "# eigvals_jax = jnp.linalg.eigvals(M_square_jax)   # eigenvalues (may be complex)\n",
    "\n",
    "# show(\"Q_jax (from QR)\", Q_jax)\n",
    "# show(\"R_jax (from QR)\", R_jax)\n",
    "# show(\"eigvals_jax\", eigvals_jax)\n",
    "\n",
    "# ------------------------------ PyTorch ------------------------------\n",
    "print(\"\\n--- PyTorch: matmul / linalg ---\")\n",
    "\n",
    "X_t3 = torch.arange(35., dtype=torch.float32).reshape(7, 5) / 10.0\n",
    "W_t3 = torch.arange(15., dtype=torch.float32).reshape(3, 5) / 10.0\n",
    "\n",
    "Y_t3 = X_t3 @ W_t3.T   # (7,3)\n",
    "show(\"Y_t3 = X @ W^T\", Y_t3)\n",
    "\n",
    "normW_t = torch.linalg.norm(W_t3)   # Frobenius norm\n",
    "print(\"Torch ||W||_F:\", normW_t.item())\n",
    "\n",
    "M_square_t = torch.arange(25., dtype=torch.float32).reshape(5, 5)\n",
    "Q_t, R_t = torch.linalg.qr(M_square_t)         # QR\n",
    "eigvals_t = torch.linalg.eigvals(M_square_t)   # complex eigenvalues\n",
    "\n",
    "show(\"Q_t (from QR)\", Q_t)\n",
    "show(\"R_t (from QR)\", R_t)\n",
    "show(\"eigvals_t\", eigvals_t)\n",
    "\n",
    "\n",
    "# =========================================================================================\n",
    "# 5) EINSUM: TRACE, DIAGONAL, OUTER, BATCHED MATMUL, BILINEAR FORM\n",
    "# =========================================================================================\n",
    "\n",
    "print(\"\\n================ 5) Einsum Patterns ================\")\n",
    "\n",
    "# ------------------------------ JAX ------------------------------\n",
    "print(\"\\n--- JAX: einsum ---\")\n",
    "\n",
    "# 1) Trace: 'ii->'\n",
    "M_jax_e = jnp.arange(16., dtype=jnp.float32).reshape(4, 4)\n",
    "trace_jax = jnp.einsum(\"ii->\", M_jax_e)\n",
    "show(\"JAX trace = einsum('ii->', M)\", trace_jax)\n",
    "\n",
    "# 2) Diagonal: 'ii->i'\n",
    "diag_jax = jnp.einsum(\"ii->i\", M_jax_e)\n",
    "show(\"JAX diag = einsum('ii->i', M)\", diag_jax)\n",
    "\n",
    "# 3) Outer product: 'i,j->ij'\n",
    "x_jax_e = jnp.arange(5., 10.)\n",
    "y_jax_e = jnp.arange(-1., 3.)\n",
    "outer_jax = jnp.einsum(\"i,j->ij\", x_jax_e, y_jax_e)\n",
    "show(\"JAX outer = einsum('i,j->ij', x, y)\", outer_jax)\n",
    "\n",
    "# 4) Batched matmul: 'bij,bjk->bik'\n",
    "As_jax = jnp.ones((3, 2, 5))\n",
    "Bs_jax = jnp.arange(3*5*4., dtype=jnp.float32).reshape(3, 5, 4)\n",
    "batched_mm_jax = jnp.einsum(\"bij,bjk->bik\", As_jax, Bs_jax)\n",
    "show(\"JAX batched_mm = einsum('bij,bjk->bik', As, Bs)\", batched_mm_jax)\n",
    "\n",
    "# 5) Bilinear form: y_b = x_b^T A x_b (per batch)\n",
    "xb_jax = jnp.arange(15., dtype=jnp.float32).reshape(3, 5)  # (B=3, N=5)\n",
    "A2_jax = jnp.eye(5, dtype=jnp.float32)                     # (5,5)\n",
    "yb_jax = jnp.einsum(\"bi,ij,bj->b\", xb_jax, A2_jax, xb_jax) # (B,)\n",
    "show(\"JAX yb = einsum('bi,ij,bj->b', xb, A2, xb)\", yb_jax)\n",
    "\n",
    "# ------------------------------ PyTorch ------------------------------\n",
    "print(\"\\n--- PyTorch: einsum ---\")\n",
    "\n",
    "M_t_e = torch.arange(16., dtype=torch.float32).reshape(4, 4)\n",
    "trace_t = torch.einsum(\"ii->\", M_t_e)\n",
    "show(\"Torch trace = einsum('ii->', M)\", trace_t)\n",
    "\n",
    "diag_t = torch.einsum(\"ii->i\", M_t_e)\n",
    "show(\"Torch diag = einsum('ii->i', M)\", diag_t)\n",
    "\n",
    "x_t_e = torch.arange(5., 10.)\n",
    "y_t_e = torch.arange(-1., 3.)\n",
    "outer_t = torch.einsum(\"i,j->ij\", x_t_e, y_t_e)\n",
    "show(\"Torch outer = einsum('i,j->ij', x, y)\", outer_t)\n",
    "\n",
    "As_t = torch.ones(3, 2, 5)\n",
    "Bs_t = torch.arange(3*5*4., dtype=torch.float32).reshape(3, 5, 4)\n",
    "batched_mm_t = torch.einsum(\"bij,bjk->bik\", As_t, Bs_t)\n",
    "show(\"Torch batched_mm = einsum('bij,bjk->bik', As, Bs)\", batched_mm_t)\n",
    "\n",
    "xb_t = torch.arange(15., dtype=torch.float32).reshape(3, 5)  # (B=3, N=5)\n",
    "A2_t = torch.eye(5, dtype=torch.float32)                     # (5,5)\n",
    "yb_t = torch.einsum(\"bi,ij,bj->b\", xb_t, A2_t, xb_t)         # (B,)\n",
    "show(\"Torch yb = einsum('bi,ij,bj->b', xb, A2, xb)\", yb_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46fb3d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ 1) GRADIENTS (SCALAR OUTPUT) ================\n",
      "\n",
      "1.1) JAX: f : R -> R\n",
      "  x0      = 1.23\n",
      "  f(x0)   = 1.0937788486480713\n",
      "  df/dx|x0: 0.5802377462387085\n",
      "\n",
      "1.1) Torch: f : R -> R\n",
      "  x0      = 1.2300000190734863\n",
      "  f(x0)   = 1.0937788486480713\n",
      "  df/dx|x0: 0.5802377462387085\n",
      "\n",
      "1.2) JAX: F : R^n -> R\n",
      "x0_vec_jax:\n",
      "[ 0.7 -1.2  0.3]\n",
      "shape: (3,) \n",
      "\n",
      "F(x0)     = 1.3355987071990967\n",
      "∇_x F(x0)_jax:\n",
      "[ 1.8648423 -1.1296424  1.2733364]\n",
      "shape: (3,) \n",
      "\n",
      "shape(x0) = (3,) shape(∇F) = (3,)\n",
      "\n",
      "1.2) Torch: F : R^n -> R\n",
      "x0_vec_t:\n",
      "tensor([ 0.7000, -1.2000,  0.3000], device='mps:0')\n",
      "shape: torch.Size([3]) \n",
      "\n",
      "F(x0)     = 1.3355987071990967\n",
      "∇_x F(x0)_torch:\n",
      "tensor([ 1.8648, -1.1296,  1.2733], device='mps:0')\n",
      "shape: torch.Size([3]) \n",
      "\n",
      "shape(x0) = torch.Size([3]) shape(∇F) = torch.Size([3])\n",
      "\n",
      "1.3) JAX: F : R^{N1×…×Nk} -> R\n",
      "X0_jax:\n",
      "[[ 0.1 -0.2  0.3]\n",
      " [ 1.   0.5 -0.7]]\n",
      "shape: (2, 3) \n",
      "\n",
      "F(X0)      = 0.9399999976158142\n",
      "∇_X F(X0)_jax:\n",
      "[[ 0.1 -0.2  0.3]\n",
      " [ 1.   0.5 -0.7]]\n",
      "shape: (2, 3) \n",
      "\n",
      "\n",
      "1.3) Torch: F : R^{N1×…×Nk} -> R\n",
      "X0_t:\n",
      "tensor([[ 0.1000, -0.2000,  0.3000],\n",
      "        [ 1.0000,  0.5000, -0.7000]], device='mps:0')\n",
      "shape: torch.Size([2, 3]) \n",
      "\n",
      "F(X0)      = 0.9399999380111694\n",
      "∇_X F(X0)_torch:\n",
      "tensor([[ 0.1000, -0.2000,  0.3000],\n",
      "        [ 1.0000,  0.5000, -0.7000]], device='mps:0')\n",
      "shape: torch.Size([2, 3]) \n",
      "\n",
      "\n",
      "Flattening equivalence (JAX): vec(∇_X F) = ∇_{vec(X)} F\n",
      "vec(X0_jax):\n",
      "[ 0.1 -0.2  0.3  1.   0.5 -0.7]\n",
      "shape: (6,) \n",
      "\n",
      "∇_{vec(X)} F_jax:\n",
      "[ 0.1 -0.2  0.3  1.   0.5 -0.7]\n",
      "shape: (6,) \n",
      "\n",
      "vec(∇_X F(X0)_jax):\n",
      "[ 0.1 -0.2  0.3  1.   0.5 -0.7]\n",
      "shape: (6,) \n",
      "\n",
      "Difference (should be ~0): [0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Flattening equivalence (Torch): vec(∇_X F) = ∇_{vec(X)} F\n",
      "vec(X0_t):\n",
      "tensor([ 0.1000, -0.2000,  0.3000,  1.0000,  0.5000, -0.7000], device='mps:0')\n",
      "shape: torch.Size([6]) \n",
      "\n",
      "∇_{vec(X)} F_torch:\n",
      "tensor([ 0.1000, -0.2000,  0.3000,  1.0000,  0.5000, -0.7000], device='mps:0')\n",
      "shape: torch.Size([6]) \n",
      "\n",
      "vec(∇_X F(X0)_torch):\n",
      "tensor([ 0.1000, -0.2000,  0.3000,  1.0000,  0.5000, -0.7000], device='mps:0')\n",
      "shape: torch.Size([6]) \n",
      "\n",
      "Difference (should be ~0): tensor([0., 0., 0., 0., 0., 0.], device='mps:0')\n",
      "\n",
      "================ 2) JACOBIANS (VECTOR OUTPUT) ================\n",
      "\n",
      "2.1) JAX: G : R^3 -> R^3\n",
      "G_jax(x0):\n",
      "[0.57311153 0.05133067 1.        ]\n",
      "shape: (3,) \n",
      "\n",
      "Jacobian via jacfwd (JAX):\n",
      "[[0.8187308  0.57311153 0.        ]\n",
      " [0.         0.9800666  1.        ]\n",
      " [1.         1.         1.        ]]\n",
      "shape: (3, 3) \n",
      "\n",
      "Jacobian via jacrev (JAX):\n",
      "[[0.8187308  0.57311153 0.        ]\n",
      " [0.         0.9800666  1.        ]\n",
      " [1.         1.         1.        ]]\n",
      "shape: (3, 3) \n",
      "\n",
      "G_jax(x0) from jvp:\n",
      "[0.57311153 0.05133067 1.        ]\n",
      "shape: (3,) \n",
      "\n",
      "JVP (J·v)_jax:\n",
      "[-0.3274923 -1.4601332 -0.5      ]\n",
      "shape: (3,) \n",
      "\n",
      "G_jax(x0) from vjp:\n",
      "[0.57311153 0.05133067 1.        ]\n",
      "shape: (3,) \n",
      "\n",
      "VJP (J^T·w)_jax:\n",
      "[2.2456193 1.1918669 1.       ]\n",
      "shape: (3,) \n",
      "\n",
      "\n",
      "2.1) Torch: G : R^3 -> R^3\n",
      "G_torch(x0):\n",
      "tensor([0.5731, 0.0513, 1.0000], device='mps:0')\n",
      "shape: torch.Size([3]) \n",
      "\n",
      "Jacobian via torch.func.jacfwd:\n",
      "tensor([[0.8187, 0.5731, 0.0000],\n",
      "        [0.0000, 0.9801, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000]], device='mps:0')\n",
      "shape: torch.Size([3, 3]) \n",
      "\n",
      "Jacobian via torch.func.jacrev:\n",
      "tensor([[0.8187, 0.5731, 0.0000],\n",
      "        [0.0000, 0.9801, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000]], device='mps:0')\n",
      "shape: torch.Size([3, 3]) \n",
      "\n",
      "G_torch(x0) from jvp:\n",
      "tensor([0.5731, 0.0513, 1.0000], device='mps:0')\n",
      "shape: torch.Size([3]) \n",
      "\n",
      "JVP (J·v)_torch:\n",
      "tensor([-0.3275, -1.4601, -0.5000], device='mps:0')\n",
      "shape: torch.Size([3]) \n",
      "\n",
      "G_torch(x0) from vjp:\n",
      "tensor([0.5731, 0.0513, 1.0000], device='mps:0')\n",
      "shape: torch.Size([3]) \n",
      "\n",
      "VJP (J^T·w)_torch:\n",
      "tensor([2.2456, 1.1919, 1.0000], device='mps:0')\n",
      "shape: torch.Size([3]) \n",
      "\n",
      "\n",
      "2.2) JAX: G : tensor -> vector\n",
      "G_tensor_jax(X0):\n",
      "[ 0.94      -2.1999998]\n",
      "shape: (2,) \n",
      "\n",
      "J_X G_tensor_jax shape: (2, 2, 3)\n",
      "∇_X G0_jax:\n",
      "[[ 0.1 -0.2  0.3]\n",
      " [ 1.   0.5 -0.7]]\n",
      "shape: (2, 3) \n",
      "\n",
      "∇_X G1_jax:\n",
      "[[ 1.   2.   3. ]\n",
      " [ 0.5 -1.   4. ]]\n",
      "shape: (2, 3) \n",
      "\n",
      "Flattened Jacobian (JAX) shape: (2, 6)\n",
      "Row0 J_flat_jax:\n",
      "[ 0.1 -0.2  0.3  1.   0.5 -0.7]\n",
      "shape: (6,) \n",
      "\n",
      "Row1 J_flat_jax:\n",
      "[ 1.   2.   3.   0.5 -1.   4. ]\n",
      "shape: (6,) \n",
      "\n",
      "\n",
      "2.2) Torch: G : tensor -> vector\n",
      "G_tensor_torch(X0):\n",
      "tensor([ 0.9400, -2.2000], device='mps:0')\n",
      "shape: torch.Size([2]) \n",
      "\n",
      "J_X G_tensor_t shape: torch.Size([2, 2, 3])\n",
      "∇_X G0_torch:\n",
      "tensor([[ 0.1000, -0.2000,  0.3000],\n",
      "        [ 1.0000,  0.5000, -0.7000]], device='mps:0')\n",
      "shape: torch.Size([2, 3]) \n",
      "\n",
      "∇_X G1_torch:\n",
      "tensor([[ 1.0000,  2.0000,  3.0000],\n",
      "        [ 0.5000, -1.0000,  4.0000]], device='mps:0')\n",
      "shape: torch.Size([2, 3]) \n",
      "\n",
      "Flattened Jacobian (Torch) shape: torch.Size([2, 6])\n",
      "Row0 J_flat_t:\n",
      "tensor([ 0.1000, -0.2000,  0.3000,  1.0000,  0.5000, -0.7000], device='mps:0')\n",
      "shape: torch.Size([6]) \n",
      "\n",
      "Row1 J_flat_t:\n",
      "tensor([ 1.0000,  2.0000,  3.0000,  0.5000, -1.0000,  4.0000], device='mps:0')\n",
      "shape: torch.Size([6]) \n",
      "\n",
      "\n",
      "2.3) JAX: multi-arg G(X, θ) -> vector\n",
      "G_multi_jax(X0, θ0):\n",
      "[0.65  5.718]\n",
      "shape: (2,) \n",
      "\n",
      "J_X_jax shape: (2, 2, 2)\n",
      "∇_X G0_jax:\n",
      "[[ 0.2 -1. ]\n",
      " [ 1.5  0.7]]\n",
      "shape: (2, 2) \n",
      "\n",
      "∇_X G1_jax:\n",
      "[[ 2.   1. ]\n",
      " [-0.6  4. ]]\n",
      "shape: (2, 2) \n",
      "\n",
      "J_θ_jax shape: (2, 2, 2)\n",
      "∇_θ G0_jax:\n",
      "[[ 1.   0.5]\n",
      " [-0.3  2. ]]\n",
      "shape: (2, 2) \n",
      "\n",
      "∇_θ G1_jax:\n",
      "[[ 0.04 -0.2 ]\n",
      " [ 0.3   0.14]]\n",
      "shape: (2, 2) \n",
      "\n",
      "\n",
      "2.3) Torch: multi-arg G(X, θ) -> vector\n",
      "G_multi_torch(X0, θ0):\n",
      "tensor([0.6500, 5.7180], device='mps:0')\n",
      "shape: torch.Size([2]) \n",
      "\n",
      "J_X_t shape: torch.Size([2, 2, 2])\n",
      "∇_X G0_torch:\n",
      "tensor([[ 0.2000, -1.0000],\n",
      "        [ 1.5000,  0.7000]], device='mps:0')\n",
      "shape: torch.Size([2, 2]) \n",
      "\n",
      "∇_X G1_torch:\n",
      "tensor([[ 2.0000,  1.0000],\n",
      "        [-0.6000,  4.0000]], device='mps:0')\n",
      "shape: torch.Size([2, 2]) \n",
      "\n",
      "J_θ_t shape: torch.Size([2, 2, 2])\n",
      "∇_θ G0_torch:\n",
      "tensor([[ 1.0000,  0.5000],\n",
      "        [-0.3000,  2.0000]], device='mps:0')\n",
      "shape: torch.Size([2, 2]) \n",
      "\n",
      "∇_θ G1_torch:\n",
      "tensor([[ 0.0400, -0.2000],\n",
      "        [ 0.3000,  0.1400]], device='mps:0')\n",
      "shape: torch.Size([2, 2]) \n",
      "\n",
      "\n",
      "================ 3) Subset-of-input grads + vmap ================\n",
      "\n",
      "3.1) JAX: subset-of-input grads (w vs (w,b))\n",
      "loss_jax = 1.4449999332427979\n",
      "∂loss/∂w_jax:\n",
      "[ 0.04999995 -1.5         3.1       ]\n",
      "shape: (3,) \n",
      "\n",
      "∂loss/∂w_jax2: [ 0.04999995 -1.5         3.1       ]   ∂loss/∂b_jax2: -0.70000005\n",
      "\n",
      "3.1) Torch: subset-of-input grads (w vs (w,b))\n",
      "loss_torch = 1.4449999332427979\n",
      "∂loss/∂w_t:\n",
      "tensor([ 0.0500, -1.5000,  3.1000], device='mps:0')\n",
      "shape: torch.Size([3]) \n",
      "\n",
      "∂loss/∂w_t2: tensor([ 0.0500, -1.5000,  3.1000], device='mps:0')   ∂loss/∂b_t2: tensor(-0.7000, device='mps:0')\n",
      "\n",
      "3.2) JAX: vmap df/dx over xs\n",
      "df/dx_jax(xs):\n",
      "[-0.81614685  0.3403023   1.          0.74030226 -0.01614684]\n",
      "shape: (5,) \n",
      "\n",
      "\n",
      "3.2) Torch: vmap df/dx over xs\n",
      "df/dx_t(xs):\n",
      "tensor([-0.8161,  0.3403,  1.0000,  0.7403, -0.0161], device='mps:0')\n",
      "shape: torch.Size([5]) \n",
      "\n",
      "JAX g(a,x) over xs:\n",
      "[-0.8322937  1.0806046  2.         1.0806046 -0.8322937]\n",
      "shape: (5,) \n",
      "\n",
      "Torch g(a,x) over xs:\n",
      "tensor([-0.8323,  1.0806,  2.0000,  1.0806, -0.8323], device='mps:0')\n",
      "shape: torch.Size([5]) \n",
      "\n",
      "\n",
      "3.2) JAX: batched ∇F_vec\n",
      "grads_batched_jax:\n",
      "[[1.         1.         1.        ]\n",
      " [1.5575826  1.1640666  0.87700415]\n",
      " [1.9703023  0.53733647 1.6568421 ]]\n",
      "shape: (3, 3) \n",
      "\n",
      "\n",
      "3.2) Torch: batched ∇F_vec\n",
      "grads_batched_t:\n",
      "tensor([[1.0000, 1.0000, 1.0000],\n",
      "        [1.5576, 1.1641, 0.8770],\n",
      "        [1.9703, 0.5373, 1.6568]], device='mps:0')\n",
      "shape: torch.Size([3, 3]) \n",
      "\n",
      "JAX batched Jacobians shape: (3, 3, 3)\n",
      "JAX J(xs_batch[0]):\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 1. 1.]]\n",
      "shape: (3, 3) \n",
      "\n",
      "Torch batched Jacobians shape: torch.Size([3, 3, 3])\n",
      "Torch J(xs_batch[0]):\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [1., 1., 1.]], device='mps:0')\n",
      "shape: torch.Size([3, 3]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========================= Gradients, Jacobians, JVP/VJP: JAX vs PyTorch =========================\n",
    "\n",
    "# In this section we write everything in a **JAX-style functional way**:\n",
    "#\n",
    "#   • JAX:      jax.grad, jax.jacfwd, jax.jacrev, jax.jvp, jax.vjp, jax.vmap\n",
    "#   • PyTorch:  torch.func.grad, torch.func.jacfwd, torch.func.jacrev,\n",
    "#               torch.func.jvp, torch.func.vjp, torch.func.vmap\n",
    "#\n",
    "# Key idea:\n",
    "#   • We treat functions as PURE maps on tensors:\n",
    "#         f(x) -> scalar\n",
    "#         F(x) -> vector\n",
    "#     and ask JAX/PyTorch to return derivatives of those functions.\n",
    "\n",
    "# Helper for printing\n",
    "def show(name, x):\n",
    "    print(f\"{name}:\")\n",
    "    print(x)\n",
    "    print(\"shape:\", getattr(x, \"shape\", \"no .shape\"), \"\\n\")\n",
    "\n",
    "# =========================================================================================\n",
    "# 1) GRADIENTS (SCALAR OUTPUT)\n",
    "# =========================================================================================\n",
    "# 1.1) f : R → R\n",
    "# 1.2) F : R^n → R\n",
    "# 1.3) F : R^{N1×…×Nk} → R (+ flatten equivalence)\n",
    "# =========================================================================================\n",
    "\n",
    "print(\"\\n================ 1) GRADIENTS (SCALAR OUTPUT) ================\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1.1) f : R → R   (JAX)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def f_scalar_jax(x):\n",
    "    \"\"\"f: R -> R   (scalar input, scalar output) in JAX.\"\"\"\n",
    "    return jnp.sin(x) + 0.1 * x**2\n",
    "\n",
    "df_dx_jax = jax.grad(f_scalar_jax)\n",
    "\n",
    "x0_jax = 1.23\n",
    "print(\"\\n1.1) JAX: f : R -> R\")\n",
    "print(\"  x0      =\", x0_jax)\n",
    "print(\"  f(x0)   =\", float(f_scalar_jax(x0_jax)))\n",
    "print(\"  df/dx|x0:\", float(df_dx_jax(x0_jax)))\n",
    "\n",
    "\n",
    "# 1.1) f : R → R   (PyTorch)\n",
    "def f_scalar_torch(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"f: R -> R   (scalar input, scalar output) in Torch.\"\"\"\n",
    "    return torch.sin(x) + 0.1 * x**2\n",
    "\n",
    "df_dx_torch = torch.func.grad(f_scalar_torch)\n",
    "\n",
    "x0_t = torch.tensor(1.23)\n",
    "print(\"\\n1.1) Torch: f : R -> R\")\n",
    "print(\"  x0      =\", x0_t.item())\n",
    "print(\"  f(x0)   =\", f_scalar_torch(x0_t).item())\n",
    "print(\"  df/dx|x0:\", df_dx_torch(x0_t).item())\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1.2) F : R^n → R   (vector input, scalar output)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def F_vec_jax(x):\n",
    "    \"\"\"\n",
    "    JAX: F: R^3 -> R.\n",
    "      F(x) = sum_i [ sin(x_i) + 0.5 * x_i^2 ] + 0.1 * (x^T A x)\n",
    "    \"\"\"\n",
    "    A = jnp.array([[2.0, -0.5, 0.0],\n",
    "                   [-0.5, 1.0, 0.3],\n",
    "                   [0.0,  0.3, 1.5]])\n",
    "    quad = x @ A @ x\n",
    "    return jnp.sum(jnp.sin(x) + 0.5 * x**2) + 0.1 * quad\n",
    "\n",
    "grad_F_vec_jax = jax.grad(F_vec_jax)\n",
    "\n",
    "x0_vec_jax = jnp.array([0.7, -1.2, 0.3])\n",
    "g_vec_jax = grad_F_vec_jax(x0_vec_jax)\n",
    "\n",
    "print(\"\\n1.2) JAX: F : R^n -> R\")\n",
    "show(\"x0_vec_jax\", x0_vec_jax)\n",
    "print(\"F(x0)     =\", float(F_vec_jax(x0_vec_jax)))\n",
    "show(\"∇_x F(x0)_jax\", g_vec_jax)\n",
    "print(\"shape(x0) =\", x0_vec_jax.shape, \"shape(∇F) =\", g_vec_jax.shape)\n",
    "\n",
    "\n",
    "def F_vec_torch(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Torch: F: R^3 -> R.\n",
    "      F(x) = sum_i [ sin(x_i) + 0.5 * x_i^2 ] + 0.1 * (x^T A x)\n",
    "    \"\"\"\n",
    "    A = torch.tensor([[2.0, -0.5, 0.0],\n",
    "                      [-0.5, 1.0, 0.3],\n",
    "                      [0.0,  0.3, 1.5]])\n",
    "    quad = x @ A @ x\n",
    "    return torch.sum(torch.sin(x) + 0.5 * x**2) + 0.1 * quad\n",
    "\n",
    "grad_F_vec_torch = torch.func.grad(F_vec_torch)\n",
    "\n",
    "x0_vec_t = torch.tensor([0.7, -1.2, 0.3])\n",
    "g_vec_t = grad_F_vec_torch(x0_vec_t)\n",
    "\n",
    "print(\"\\n1.2) Torch: F : R^n -> R\")\n",
    "show(\"x0_vec_t\", x0_vec_t)\n",
    "print(\"F(x0)     =\", F_vec_torch(x0_vec_t).item())\n",
    "show(\"∇_x F(x0)_torch\", g_vec_t)\n",
    "print(\"shape(x0) =\", x0_vec_t.shape, \"shape(∇F) =\", g_vec_t.shape)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1.3) F : R^{N1×…×Nk} → R   (tensor input, scalar output)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def F_tensor_jax(X):\n",
    "    \"\"\"\n",
    "    JAX: F(X) = 1/2 ||X||^2  (Frobenius norm squared).\n",
    "    X ∈ R^{2×3} here.\n",
    "    \"\"\"\n",
    "    return 0.5 * jnp.sum(X**2)\n",
    "\n",
    "grad_F_tensor_jax = jax.grad(F_tensor_jax)\n",
    "\n",
    "X0_jax = jnp.array([[0.1, -0.2, 0.3],\n",
    "                    [1.0,  0.5, -0.7]])\n",
    "\n",
    "g_tensor_jax = grad_F_tensor_jax(X0_jax)\n",
    "\n",
    "print(\"\\n1.3) JAX: F : R^{N1×…×Nk} -> R\")\n",
    "show(\"X0_jax\", X0_jax)\n",
    "print(\"F(X0)      =\", float(F_tensor_jax(X0_jax)))\n",
    "show(\"∇_X F(X0)_jax\", g_tensor_jax)\n",
    "\n",
    "\n",
    "def F_tensor_torch(X: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Torch: F(X) = 1/2 ||X||^2, X ∈ R^{2×3} here.\n",
    "    \"\"\"\n",
    "    return 0.5 * torch.sum(X**2)\n",
    "\n",
    "grad_F_tensor_torch = torch.func.grad(F_tensor_torch)\n",
    "\n",
    "X0_t = torch.tensor([[0.1, -0.2, 0.3],\n",
    "                     [1.0,  0.5, -0.7]])\n",
    "\n",
    "g_tensor_t = grad_F_tensor_torch(X0_t)\n",
    "\n",
    "print(\"\\n1.3) Torch: F : R^{N1×…×Nk} -> R\")\n",
    "show(\"X0_t\", X0_t)\n",
    "print(\"F(X0)      =\", F_tensor_torch(X0_t).item())\n",
    "show(\"∇_X F(X0)_torch\", g_tensor_t)\n",
    "\n",
    "# Flattening equivalence\n",
    "def F_flat_jax(x_flat):\n",
    "    return 0.5 * jnp.sum(x_flat**2)\n",
    "\n",
    "grad_F_flat_jax = jax.grad(F_flat_jax)\n",
    "x0_flat_jax = X0_jax.reshape(-1)\n",
    "g_flat_jax = grad_F_flat_jax(x0_flat_jax)\n",
    "\n",
    "print(\"\\nFlattening equivalence (JAX): vec(∇_X F) = ∇_{vec(X)} F\")\n",
    "show(\"vec(X0_jax)\", x0_flat_jax)\n",
    "show(\"∇_{vec(X)} F_jax\", g_flat_jax)\n",
    "show(\"vec(∇_X F(X0)_jax)\", g_tensor_jax.reshape(-1))\n",
    "print(\"Difference (should be ~0):\", g_flat_jax - g_tensor_jax.reshape(-1))\n",
    "\n",
    "def F_flat_torch(x_flat: torch.Tensor) -> torch.Tensor:\n",
    "    return 0.5 * torch.sum(x_flat**2)\n",
    "\n",
    "grad_F_flat_torch = torch.func.grad(F_flat_torch)\n",
    "x0_flat_t = X0_t.reshape(-1)\n",
    "g_flat_t = grad_F_flat_torch(x0_flat_t)\n",
    "\n",
    "print(\"\\nFlattening equivalence (Torch): vec(∇_X F) = ∇_{vec(X)} F\")\n",
    "show(\"vec(X0_t)\", x0_flat_t)\n",
    "show(\"∇_{vec(X)} F_torch\", g_flat_t)\n",
    "show(\"vec(∇_X F(X0)_torch)\", g_tensor_t.reshape(-1))\n",
    "print(\"Difference (should be ~0):\", g_flat_t - g_tensor_t.reshape(-1))\n",
    "\n",
    "\n",
    "# =========================================================================================\n",
    "# 2) JACOBIANS (VECTOR OUTPUT) + JVP / VJP\n",
    "# =========================================================================================\n",
    "# 2.1) G : R^n → R^m\n",
    "# 2.2) G : tensor → vector\n",
    "# 2.3) G : (X, θ) → vector (multi-arg)\n",
    "# =========================================================================================\n",
    "\n",
    "print(\"\\n================ 2) JACOBIANS (VECTOR OUTPUT) ================\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2.1) G : R^n → R^m (vector → vector) + JVP/VJP\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def G_jax(x):\n",
    "    \"\"\"\n",
    "    JAX: G: R^3 -> R^3, components:\n",
    "      G0 = x0 * exp(x1)\n",
    "      G1 = sin(x1) + x2^2\n",
    "      G2 = x0 + x1 + x2\n",
    "    \"\"\"\n",
    "    x0, x1, x2 = x\n",
    "    return jnp.array([\n",
    "        x0 * jnp.exp(x1),\n",
    "        jnp.sin(x1) + x2**2,\n",
    "        x0 + x1 + x2\n",
    "    ])\n",
    "\n",
    "x0_jax_vec = jnp.array([0.7, -0.2, 0.5])\n",
    "J_fwd_jax = jax.jacfwd(G_jax)(x0_jax_vec)   # forward-mode\n",
    "J_rev_jax = jax.jacrev(G_jax)(x0_jax_vec)   # reverse-mode\n",
    "\n",
    "print(\"\\n2.1) JAX: G : R^3 -> R^3\")\n",
    "show(\"G_jax(x0)\", G_jax(x0_jax_vec))\n",
    "show(\"Jacobian via jacfwd (JAX)\", J_fwd_jax)\n",
    "show(\"Jacobian via jacrev (JAX)\", J_rev_jax)\n",
    "\n",
    "# JVP: J(x0) · v\n",
    "v_jax = jnp.array([1.0, -2.0, 0.5])\n",
    "y_jax, Jv_jax = jax.jvp(G_jax, (x0_jax_vec,), (v_jax,))\n",
    "show(\"G_jax(x0) from jvp\", y_jax)\n",
    "show(\"JVP (J·v)_jax\", Jv_jax)\n",
    "\n",
    "# VJP: J(x0)^T · w\n",
    "w_jax = jnp.array([0.3, -1.0, 2.0])\n",
    "G_x0_val_jax, vjp_fun_jax = jax.vjp(G_jax, x0_jax_vec)\n",
    "(wJ_jax,) = vjp_fun_jax(w_jax)\n",
    "show(\"G_jax(x0) from vjp\", G_x0_val_jax)\n",
    "show(\"VJP (J^T·w)_jax\", wJ_jax)\n",
    "\n",
    "\n",
    "def G_torch(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Torch: G: R^3 -> R^3, same as JAX version.\n",
    "    \"\"\"\n",
    "    x0, x1, x2 = x\n",
    "    return torch.stack([\n",
    "        x0 * torch.exp(x1),\n",
    "        torch.sin(x1) + x2**2,\n",
    "        x0 + x1 + x2\n",
    "    ])\n",
    "\n",
    "x0_t_vec = torch.tensor([0.7, -0.2, 0.5])\n",
    "J_fwd_t = torch.func.jacfwd(G_torch)(x0_t_vec)\n",
    "J_rev_t = torch.func.jacrev(G_torch)(x0_t_vec)\n",
    "\n",
    "print(\"\\n2.1) Torch: G : R^3 -> R^3\")\n",
    "show(\"G_torch(x0)\", G_torch(x0_t_vec))\n",
    "show(\"Jacobian via torch.func.jacfwd\", J_fwd_t)\n",
    "show(\"Jacobian via torch.func.jacrev\", J_rev_t)\n",
    "\n",
    "# JVP: J(x0) · v\n",
    "v_t = torch.tensor([1.0, -2.0, 0.5])\n",
    "y_t, Jv_t = torch.func.jvp(G_torch, (x0_t_vec,), (v_t,))\n",
    "show(\"G_torch(x0) from jvp\", y_t)\n",
    "show(\"JVP (J·v)_torch\", Jv_t)\n",
    "\n",
    "# VJP: J(x0)^T · w\n",
    "w_t = torch.tensor([0.3, -1.0, 2.0])\n",
    "G_x0_val_t, vjp_fun_t = torch.func.vjp(G_torch, x0_t_vec)\n",
    "(wJ_t,) = vjp_fun_t(w_t)\n",
    "show(\"G_torch(x0) from vjp\", G_x0_val_t)\n",
    "show(\"VJP (J^T·w)_torch\", wJ_t)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2.2) G : R^{N1×…×Nk} → R^m  (tensor → vector)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def G_tensor_jax(X):\n",
    "    \"\"\"\n",
    "    JAX: X ∈ R^{2×3}, G(X) ∈ R^2\n",
    "      G0 = 0.5 * ||X||^2\n",
    "      G1 = ⟨A, X⟩\n",
    "    \"\"\"\n",
    "    A = jnp.array([[1.0,  2.0,  3.0],\n",
    "                   [0.5, -1.0,  4.0]])\n",
    "    G0 = 0.5 * jnp.sum(X**2)\n",
    "    G1 = jnp.sum(A * X)\n",
    "    return jnp.array([G0, G1])\n",
    "\n",
    "X0_jax_T = jnp.array([[0.1, -0.2, 0.3],\n",
    "                      [1.0,  0.5, -0.7]])\n",
    "\n",
    "J_tensor_jax = jax.jacrev(G_tensor_jax)(X0_jax_T)  # (m=2, 2,3)\n",
    "\n",
    "print(\"\\n2.2) JAX: G : tensor -> vector\")\n",
    "show(\"G_tensor_jax(X0)\", G_tensor_jax(X0_jax_T))\n",
    "print(\"J_X G_tensor_jax shape:\", J_tensor_jax.shape)\n",
    "show(\"∇_X G0_jax\", J_tensor_jax[0])\n",
    "show(\"∇_X G1_jax\", J_tensor_jax[1])\n",
    "\n",
    "# Flattened equivalence\n",
    "def G_tensor_flat_jax(x_flat):\n",
    "    X = x_flat.reshape(2, 3)\n",
    "    return G_tensor_jax(X)\n",
    "\n",
    "x0_flat_T_jax = X0_jax_T.reshape(-1)\n",
    "J_flat_jax = jax.jacrev(G_tensor_flat_jax)(x0_flat_T_jax)  # (2,6)\n",
    "print(\"Flattened Jacobian (JAX) shape:\", J_flat_jax.shape)\n",
    "show(\"Row0 J_flat_jax\", J_flat_jax[0])\n",
    "show(\"Row1 J_flat_jax\", J_flat_jax[1])\n",
    "\n",
    "\n",
    "def G_tensor_torch(X: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Torch: X ∈ R^{2×3}, G(X) ∈ R^2\n",
    "      G0 = 0.5 * ||X||^2\n",
    "      G1 = ⟨A, X⟩\n",
    "    \"\"\"\n",
    "    A = torch.tensor([[1.0,  2.0,  3.0],\n",
    "                      [0.5, -1.0,  4.0]])\n",
    "    G0 = 0.5 * torch.sum(X**2)\n",
    "    G1 = torch.sum(A * X)\n",
    "    return torch.stack([G0, G1])\n",
    "\n",
    "X0_t_T = torch.tensor([[0.1, -0.2, 0.3],\n",
    "                       [1.0,  0.5, -0.7]])\n",
    "\n",
    "J_tensor_t = torch.func.jacrev(G_tensor_torch)(X0_t_T)  # (2,2,3)\n",
    "\n",
    "print(\"\\n2.2) Torch: G : tensor -> vector\")\n",
    "show(\"G_tensor_torch(X0)\", G_tensor_torch(X0_t_T))\n",
    "print(\"J_X G_tensor_t shape:\", J_tensor_t.shape)\n",
    "show(\"∇_X G0_torch\", J_tensor_t[0])\n",
    "show(\"∇_X G1_torch\", J_tensor_t[1])\n",
    "\n",
    "def G_tensor_flat_torch(x_flat: torch.Tensor) -> torch.Tensor:\n",
    "    X = x_flat.reshape(2, 3)\n",
    "    return G_tensor_torch(X)\n",
    "\n",
    "x0_flat_T_t = X0_t_T.reshape(-1)\n",
    "J_flat_t = torch.func.jacrev(G_tensor_flat_torch)(x0_flat_T_t)  # (2,6)\n",
    "print(\"Flattened Jacobian (Torch) shape:\", J_flat_t.shape)\n",
    "show(\"Row0 J_flat_t\", J_flat_t[0])\n",
    "show(\"Row1 J_flat_t\", J_flat_t[1])\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2.3) G : (X, θ) → R^m  (multi-arg: X and θ)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def G_multi_jax(X, theta):\n",
    "    \"\"\"\n",
    "    JAX: X, θ ∈ R^{2×2}, G ∈ R^2:\n",
    "      G0 = ⟨X, θ⟩\n",
    "      G1 = ||X||^2 + 0.1 ||θ||^2\n",
    "    \"\"\"\n",
    "    G0 = jnp.sum(X * theta)\n",
    "    G1 = jnp.sum(X**2) + 0.1 * jnp.sum(theta**2)\n",
    "    return jnp.array([G0, G1])\n",
    "\n",
    "X0_jax_M = jnp.array([[1.0,  0.5],\n",
    "                      [-0.3, 2.0]])\n",
    "theta0_jax = jnp.array([[0.2, -1.0],\n",
    "                        [1.5,  0.7]])\n",
    "\n",
    "print(\"\\n2.3) JAX: multi-arg G(X, θ) -> vector\")\n",
    "show(\"G_multi_jax(X0, θ0)\", G_multi_jax(X0_jax_M, theta0_jax))\n",
    "\n",
    "J_X_jax = jax.jacrev(G_multi_jax, argnums=0)(X0_jax_M, theta0_jax)\n",
    "J_theta_jax = jax.jacrev(G_multi_jax, argnums=1)(X0_jax_M, theta0_jax)\n",
    "\n",
    "print(\"J_X_jax shape:\", J_X_jax.shape)\n",
    "show(\"∇_X G0_jax\", J_X_jax[0])\n",
    "show(\"∇_X G1_jax\", J_X_jax[1])\n",
    "\n",
    "print(\"J_θ_jax shape:\", J_theta_jax.shape)\n",
    "show(\"∇_θ G0_jax\", J_theta_jax[0])\n",
    "show(\"∇_θ G1_jax\", J_theta_jax[1])\n",
    "\n",
    "\n",
    "def G_multi_torch(X: torch.Tensor, theta: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Torch: X, θ ∈ R^{2×2}, G ∈ R^2:\n",
    "      G0 = ⟨X, θ⟩\n",
    "      G1 = ||X||^2 + 0.1 ||θ||^2\n",
    "    \"\"\"\n",
    "    G0 = torch.sum(X * theta)\n",
    "    G1 = torch.sum(X**2) + 0.1 * torch.sum(theta**2)\n",
    "    return torch.stack([G0, G1])\n",
    "\n",
    "X0_t_M = torch.tensor([[1.0,  0.5],\n",
    "                       [-0.3, 2.0]])\n",
    "theta0_t = torch.tensor([[0.2, -1.0],\n",
    "                         [1.5,  0.7]])\n",
    "\n",
    "print(\"\\n2.3) Torch: multi-arg G(X, θ) -> vector\")\n",
    "show(\"G_multi_torch(X0, θ0)\", G_multi_torch(X0_t_M, theta0_t))\n",
    "\n",
    "J_X_t = torch.func.jacrev(G_multi_torch, argnums=0)(X0_t_M, theta0_t)\n",
    "J_theta_t = torch.func.jacrev(G_multi_torch, argnums=1)(X0_t_M, theta0_t)\n",
    "\n",
    "print(\"J_X_t shape:\", J_X_t.shape)\n",
    "show(\"∇_X G0_torch\", J_X_t[0])\n",
    "show(\"∇_X G1_torch\", J_X_t[1])\n",
    "\n",
    "print(\"J_θ_t shape:\", J_theta_t.shape)\n",
    "show(\"∇_θ G0_torch\", J_theta_t[0])\n",
    "show(\"∇_θ G1_torch\", J_theta_t[1])\n",
    "\n",
    "\n",
    "# =========================================================================================\n",
    "# 3) GRADIENTS W.R.T. SUBSET OF INPUTS + SIMPLE BATCHING (VMAP)\n",
    "# =========================================================================================\n",
    "\n",
    "print(\"\\n================ 3) Subset-of-input grads + vmap ================\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3.1) Gradient w.r.t subset of inputs (linear regression loss)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def lin_loss_jax(w, b, x, y):\n",
    "    \"\"\"\n",
    "    JAX: simple linear regression.\n",
    "      yhat = x @ w + b\n",
    "      loss = mean((yhat - y)^2)\n",
    "    \"\"\"\n",
    "    yhat = x @ w + b\n",
    "    return jnp.mean((yhat - y)**2)\n",
    "\n",
    "grad_w_only_jax = jax.grad(lin_loss_jax, argnums=0)\n",
    "grad_w_and_b_jax = jax.grad(lin_loss_jax, argnums=(0, 1))\n",
    "\n",
    "w_jax = jnp.array([1.0, -2.0, 0.5])\n",
    "b_jax = 0.3\n",
    "x_jax_data = jnp.array([[1.0, 0.0, 2.0],\n",
    "                        [0.5, 1.0, -1.0]])\n",
    "y_jax_data = jnp.array([1.5, -0.2])\n",
    "\n",
    "print(\"\\n3.1) JAX: subset-of-input grads (w vs (w,b))\")\n",
    "print(\"loss_jax =\", float(lin_loss_jax(w_jax, b_jax, x_jax_data, y_jax_data)))\n",
    "dw_jax = grad_w_only_jax(w_jax, b_jax, x_jax_data, y_jax_data)\n",
    "show(\"∂loss/∂w_jax\", dw_jax)\n",
    "\n",
    "dw_jax2, db_jax2 = grad_w_and_b_jax(w_jax, b_jax, x_jax_data, y_jax_data)\n",
    "print(\"∂loss/∂w_jax2:\", dw_jax2, \"  ∂loss/∂b_jax2:\", db_jax2)\n",
    "\n",
    "\n",
    "def lin_loss_torch(w: torch.Tensor,\n",
    "                   b: torch.Tensor,\n",
    "                   x: torch.Tensor,\n",
    "                   y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Torch: simple linear regression.\n",
    "      yhat = x @ w + b\n",
    "      loss = mean((yhat - y)^2)\n",
    "    \"\"\"\n",
    "    yhat = x @ w + b\n",
    "    return torch.mean((yhat - y)**2)\n",
    "\n",
    "grad_w_only_t = torch.func.grad(lin_loss_torch, argnums=0)\n",
    "grad_w_and_b_t = torch.func.grad(lin_loss_torch, argnums=(0, 1))\n",
    "\n",
    "w_t_reg = torch.tensor([1.0, -2.0, 0.5])\n",
    "b_t_reg = torch.tensor(0.3)\n",
    "x_t_data = torch.tensor([[1.0, 0.0, 2.0],\n",
    "                         [0.5, 1.0, -1.0]])\n",
    "y_t_data = torch.tensor([1.5, -0.2])\n",
    "\n",
    "print(\"\\n3.1) Torch: subset-of-input grads (w vs (w,b))\")\n",
    "print(\"loss_torch =\", lin_loss_torch(w_t_reg, b_t_reg, x_t_data, y_t_data).item())\n",
    "dw_t = grad_w_only_t(w_t_reg, b_t_reg, x_t_data, y_t_data)\n",
    "show(\"∂loss/∂w_t\", dw_t)\n",
    "\n",
    "dw_t2, db_t2 = grad_w_and_b_t(w_t_reg, b_t_reg, x_t_data, y_t_data)\n",
    "print(\"∂loss/∂w_t2:\", dw_t2, \"  ∂loss/∂b_t2:\", db_t2)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3.2) vmap: batch scalar derivatives and vector gradients\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Reuse scalar f and its grad from Section 1.1\n",
    "\n",
    "xs_jax = jnp.linspace(-2.0, 2.0, 5)  # (-2,-1,0,1,2)\n",
    "df_dx_batched_jax = jax.vmap(df_dx_jax)\n",
    "print(\"\\n3.2) JAX: vmap df/dx over xs\")\n",
    "show(\"df/dx_jax(xs)\", df_dx_batched_jax(xs_jax))\n",
    "\n",
    "\n",
    "xs_t = torch.linspace(-2.0, 2.0, steps=5)\n",
    "df_dx_batched_t = torch.func.vmap(df_dx_torch)\n",
    "print(\"\\n3.2) Torch: vmap df/dx over xs\")\n",
    "show(\"df/dx_t(xs)\", df_dx_batched_t(xs_t))\n",
    "\n",
    "\n",
    "# Batch a multi-arg function g(a, x) = a cos(x) over x only\n",
    "def g_jax(a, x):\n",
    "    return a * jnp.cos(x)\n",
    "\n",
    "a_jax = 2.0\n",
    "g_batched_jax = jax.vmap(lambda x: g_jax(a_jax, x))\n",
    "show(\"JAX g(a,x) over xs\", g_batched_jax(xs_jax))\n",
    "\n",
    "\n",
    "def g_torch(a: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "    return a * torch.cos(x)\n",
    "\n",
    "a_t = torch.tensor(2.0)\n",
    "g_batched_t = torch.func.vmap(lambda x: g_torch(a_t, x))\n",
    "show(\"Torch g(a,x) over xs\", g_batched_t(xs_t))\n",
    "\n",
    "\n",
    "# Batched vector gradient: vmap over grad(F_vec)\n",
    "xs_batch_jax = jnp.stack([\n",
    "    jnp.array([0.0,  0.0,  0.0]),\n",
    "    jnp.array([0.5,  0.2, -0.1]),\n",
    "    jnp.array([1.0, -0.3,  0.7]),\n",
    "], axis=0)  # (B,3)\n",
    "\n",
    "grads_batched_jax = jax.vmap(grad_F_vec_jax)(xs_batch_jax)\n",
    "print(\"\\n3.2) JAX: batched ∇F_vec\")\n",
    "show(\"grads_batched_jax\", grads_batched_jax)\n",
    "\n",
    "\n",
    "xs_batch_t = torch.stack([\n",
    "    torch.tensor([0.0,  0.0,  0.0]),\n",
    "    torch.tensor([0.5,  0.2, -0.1]),\n",
    "    torch.tensor([1.0, -0.3,  0.7]),\n",
    "], dim=0)  # (B,3)\n",
    "\n",
    "grads_batched_t = torch.func.vmap(grad_F_vec_torch)(xs_batch_t)\n",
    "print(\"\\n3.2) Torch: batched ∇F_vec\")\n",
    "show(\"grads_batched_t\", grads_batched_t)\n",
    "\n",
    "\n",
    "# Batched Jacobians: vmap over jacrev(G)\n",
    "batched_J_jax = jax.vmap(jax.jacrev(G_jax))(xs_batch_jax)   # (B,3,3)\n",
    "print(\"JAX batched Jacobians shape:\", batched_J_jax.shape)\n",
    "show(\"JAX J(xs_batch[0])\", batched_J_jax[0])\n",
    "\n",
    "batched_J_t = torch.func.vmap(torch.func.jacrev(G_torch))(xs_batch_t)  # (B,3,3)\n",
    "print(\"Torch batched Jacobians shape:\", batched_J_t.shape)\n",
    "show(\"Torch J(xs_batch[0])\", batched_J_t[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sci-dev)",
   "language": "python",
   "name": "sci-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
